\documentclass[11pt]{article}
\usepackage{lscape}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{float}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{bm}
\usepackage{gensymb}
\allowdisplaybreaks[4]
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{setspace}
\usepackage{siunitx}
\usepackage{enumitem}
\usepackage{dsfont}

\usepackage{graphics}
\usepackage[noend]{algorithmic}
\usepackage[utf8x]{inputenc}
\usepackage{bm}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    citecolor = blue,
    linkcolor=blue,
    filecolor=magenta,           
    urlcolor=cyan,
}


\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{pro}{Property}
\newtheorem{cor}{Corollary}
\newtheorem{ass}{Assumption}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{exmp}{Example}
\newtheorem{rmk}{Remark}



\usepackage[labelfont=bf]{caption}

\setcounter{table}{1}
 % \usepackage[labelformat=empty]{ caption}
\usepackage{multirow}
\usepackage{tabularx}

\def\fixme#1#2{\textbf{[FIXME (#1): #2]}}

 

\newcommand*{\KeepStyleUnderBrace}[1]{%f
  \mathop{%
    \mathchoice
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}
% \usepackage[initials]{amsrefs}
%\usepackage{amsaddr}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs=true}


\usepackage{hyperref}
\hypersetup{colorlinks=true}
\usepackage[parfill]{parskip}
\usepackage{bm}
\onehalfspacing
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%             Math Symbols
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%               Bold Math
\input macros.tex
\def\refer#1{\emph{\color{blue}#1}}

\usepackage{sectsty}
\sectionfont{\fontsize{12}{15}\selectfont}

\newcommand*{\QEDB}{\hfill\ensuremath{\square}}%


%\author{%
%Yuchen Zeng \\
%University of Wisconsin -- Madison\\
 %\texttt{yzeng58@wisc.edu} \\
%\And
%Miaoyan Wang \\
%University of Wisconsin -- Madison\\
%\texttt{miaoyan.wang@wisc.edu} \\
%}

\begin{document}
\begin{center}
{\bf \large Paper Sketch for AISTATS\\
Tentative title: ``Binary tensor regression with multi-mode features''}\\
first draft on 08/13, updated on 08/22
\end{center}

\section{Preliminaries}


We use lower-case letters $(a, b, \ldots)$ for scalars and vectors, upper-case boldface letters $(\mA, \mB, \ldots)$ for matrices, and calligraphy letter $(\tA,\tB,\ldots)$ for tensors of order 3 or greater. Let $\tY \in \mathbb{R}^{d_1\times \cdots \times d_K}$ denote an order-$K$ $(d_1,\ldots,d_K)$-dimensional tensor. We say that an event $A$ occurs ``with very high probability'' if $\mathbb{P}(A)$ tends to 1 faster than any polynomial of $d_{\min}=\min\{d_1, . . . , d_K\}$. We use $\mS^{d-1}=\{\mx\in\mathbb{R}^d: \norm{\mx}=1\}$ to denote the Euclidean unit sphere in dimension $d$.\\


\begin{pro}\label{eq:defn}
Let $\mX\in\mathbb{R}^{d\times p}$ be a rank-$r$ matrix with $p\leq d$. The SVD of $\mX$ can be expressed as $\mX=\mP\Delta\mQ^T$, where $\mP\in\mathbb{R}^{d\times r}$ and $\mQ\in\mathbb{R}^{p\times r}$ consist of, respectively, the left and right singular vectors, and $\Delta\in\mathbb{R}^{s\times r}$ is the diagonal matrix consisting of non-zero singular values. The following properties hold:
\begin{enumerate}
%\item $(\mX^T\mX)^{1/2}=\mQ\Delta\in\mathbb{R}^{p\times s}$,
\item $(\mX^T\mX)^{-1/2}=\mQ\Delta^{-1}\mQ^T$.
\item Let $\tilde \mX=\mX(\mX^T\mX)^{-1/2}$. Then $\tilde \mX=\mP\mQ^{T}$.
%\item The transpose of $(\mX^T\mX)^{1/2}$ is the Moore-Penrose inverse of $(\mX^T\mX)^{-1/2}$. 
%\item Let $\tilde \mX=\mX(\mX^T\mX)^{-1/2}$. Then $\tilde \mX=\mP$, so the matrix $\tilde \mX$  has orthogonal columns. 
\item $\tilde \mX^T\mX=\mQ\Delta\mQ^T$.
\end{enumerate}
\end{pro}






\section{Results}
Suppose we observe an order-$K$ binary tensor $\tY\in\{0,1\}^{d_1\times \cdots \times d_K}$, along with a set of covariate matrices $\mX_k\in\mathbb{R}^{d_k\times p_k}$ for $k=1,\ldots,K$. Consider a tensor regression model:
\begin{equation}\label{eq:model}
\text{logit}(\mathbb{E}(\tY))=\tB\times_1\mX_1\times_2\cdots\times_K \mX_K,
\end{equation}
where $\tB\in \mathbb{R}^{p_1\times \cdots \times p_K}$ is a coefficient tensor of interest. Furthermore, the tensor $\tB$ is assumed to (i) be entrywise bounded,  and (ii) admit a low-rank Tucker decomposition; that is, $\text{rank}(\tB)=\mr\equiv(r_1,\ldots,r_K)^T$, where $r_k\leq p_k\leq d_k$. The parameter space we consider is
\[
\tP=\tP(\mr,\alpha)=\{ \tB \in \mathbb{R}^{p_1\times \cdots \times p_K}\colon \text{rank}(\tB)\leq \mr, \ \text{and} \ \mnormSize{}{\tB}\leq \alpha\}.
\]

In the following analysis, we assume both the multilinear rank $\mr$ and entrywise bound $\alpha$ are known. The adaptation of unknown rank will be addressed in the next note. 

\begin{rmk}
Model~\eqref{eq:model} incorporates the following examples as special cases:

(1) {\bf Binary tensor decomposition}. In the absence of side information, set $\mX=\mI_k$ to be identity matrix and $p_k=d_k$ for $k=1,\ldots,K$. Then the model~\eqref{eq:model} reduces to unsupervised binary tensor decomposition.

(2) {\bf Network link prediction model}. Suppose $K=2$ and $\mX_1=\mX_2$. Then the model~\eqref{eq:model} reduced to the matrix logistic model [Baldin and Berthet, 2018] that is commonly used in the network analysis:
\[
\text{logit}(\mathbb{E}(\mY))=\mX^T\mB\mX,\quad \text{where}\quad \text{rank}(\mB)\leq r. 
\]

(3) {\bf Semi-supervised decomposition}. Suppose the covariate information is available only for a subset of modes. Without loss of generality, suppose the covariates $\mX_k\neq \mI$ are available in modes $1,\ldots,L$, where $L< K$. Then the model~\eqref{eq:model} reduces to a semi-supervised decomposition model:
\[
\text{logit}(\mathbb{E}(\tY))=\KeepStyleUnderBrace{\tB}_{\text{$\in\mathbb{R}^{{\color{red}p_1\times \cdots\times p_L}\times d_{L+1}\times \cdots \times  d_K}$}} \times_1 \KeepStyleUnderBrace{\mX_1}_{\text{$\in \mathbb{R}^{d_1\times {\color{red}p_1}}$}}\times_2\cdots\times_L\KeepStyleUnderBrace{\mX_L}_{\text{$\in \mathbb{R}^{d_L \times {\color{red}p_L}}$}}.
\] 
\end{rmk}

For parsimony, we do not distinguish modes with available side information from those without side information. We focus on the general tensor regression model~\eqref{eq:model} with mild assumption on $\{\mX_k\}$. Specifically, the covariates $\{\mX_k\}$ are assumed to satisfy the following restricted isometry property (RIP) assumption. 

\begin{ass}[Restricted Isometry Property]\label{ass:RIP}
Let $d=\prod_k d_k$. The covariates $\{\mX_k\}$ are called to satisfy the RIP condition if there exists a positive constant $\delta_{\mr,\alpha}\in(0,1)$ such that 
\[
d(1-\delta_{\mr,\alpha})\Fnorm{\tB}^2\leq \Fnorm{\tB \times_1\mX_1\times_2\cdots\times_K\mX_K}^2\leq d(1+\delta_{\mr,\alpha})\Fnorm{\tB}^2,
\]
holds for all tensors $\tB\in \tP(\mr,\alpha)$ in the parameter space. 
\end{ass}
\begin{rmk}
The RIP assumption requires the covariates at each of the modes are nearly orthonormal when restricted to the desired parameter space. The RIP condition is a relatively mild assumption on the covariates. In particular, both fixed design and Gaussian random design satisfy the RIP condition. \end{rmk}

\begin{exmp} [Gaussian Random design] Suppose $\mX_k \in\mathbb{R}^{d_k\times p_k}$ are random design matrices with i.i.d.\ standard Gaussian entries, where ${p_k\over d_k}=\lambda_k\in(0,1)$ for all $k=1,\ldots,K$. Then, with very high probability, $\{\mX_k\}$ satisfy the RIP condition. In particular, the RIP constant can be chosen as
\[
\delta_{\mr,\alpha}=\max\left\{ -1+\prod_k \sqrt{1+\lambda_k},\ 1-\prod_k \sqrt{1-\lambda_k}\right\} \in (0,1)}.
\]
\end{exmp}

\begin{exmp}[Fixed design] Suppose $\mX_k \in\mathbb{R}^{d_k\times p_k}$ are deterministic design matrices with full rank. Then, $\{\mX_k\}$ (upon proper rescaling) satisfy the RIP condition.
\end{exmp}

\begin{thm} [Main Results]
\label{thm:main}
Consider a tensor regression model~\eqref{eq:model} with $\tY\in\{0,1\}^{d_1\times \cdots\times d_K}$ the response and $\mX_k\in\mathbb{R}^{d_k\times p_k}$ the mode-$k$ covariates. 
Let $\hat \tB_{\text{MLE}}$ be the restricted rank-$\mr$ maximum likelihood estimate of the coefficient tensor, where $\mr=(r_1,\ldots,r_K)$,
\[
\hat \tB_{\text{MLE}}=\argmin_{\tB\colon \text{rank}(\tB)=\mr, \mnormSize{}{\tB}\leq \alpha}} \text{Log-lik}\ (\tB; \tY, \{\mX_k\}).
\]
Suppose the covariates $\mX_k$ satisfy the RIP condition with RIP constant $\delta \in(0,1)$. 
Then, with very high probability,
\[
\Fnorm{\hat \tB_{\text{MLE}}-\tB_{\text{true}}}\leq {C_\alpha\over \sqrt{\prod_k d_k}} \sqrt{ {(1+\delta_{2\mr,2\alpha})\over (1-\delta_{2\mr,2\alpha})^2}{\prod^K_{k=1} r_k\over r_{\max}} \sum^K_{k=1} p_k},
\]
where $C_\alpha>0$ is a constant independent of the tensor dimension or rank. 
\end{thm}

\begin{thm}[KL-Divergence and Hellinger Loss]
See Zhuoyan's note ``Evidence theory on prediction error'' (08/09) and Jiaxin's note ``Boundaries for different prediction error metrics'' (08/09). 
\end{thm}

\section{Experiments}
{\bf Algorithm sketch}. Please refer to earlier notes for algorithm. \\
{\bf Rank selection}: We propose to use Bayesian information criterion (BIC) and choose the rank that minimizes BIC; i.e.
\[
\hat \mr=\argmin_{\mr=(r_1,\ldots,r_K)} BIC(\mr)=\argmin_{\mr=(r_1,\ldots,r_K)}\left[-2\text{Log-lik}(\hat \Theta)+p_e(r_1,\ldots,r_K)\log \left(\prod_k d_k\right) \right],
\]
where $p_e(r_1,\ldots,r_K)\stackrel{\text{def}}{=}\sum_k (d_k-1)r_k+\prod_k r_k$ is the effective number of parameters in the model. 

{\bf Performance.} There are several issues summarized in the note ``Summary\_of\_last\_semester\_
and\_summer\_plan.pdf''. The main issues are
\begin{itemize}
\item The choice of distribution to generate the core tensor. Bad estimation when the core is generated i.i.d.\ $N(0,1)$, $N(10,1)$, $\text{Unif}[0,1]$ or $\text{Unif}[0,10]$.
\item When to stop? ``It is reported that when we run more iterations, the result sometime seems not better or even worse.''
\item Unbounded estimation. ``Sometimes, when we check the real scale of $U$, there may some entries with extremely large values''.
\end{itemize}
We have proposed to impose an infinity-norm constraint to the optimization. Which of the following leads to the best result?
\begin{enumerate}
\item Update the core tensor to the feasible region via conjugate gradient solver. (good MSE, but haven not checked its performance on ``bad'' distributions. )
\item Update the core by global downscaling.
\item Update the factor matrixes by backward linear search (computationally slow).
\end{enumerate}


MSE exhibits good agreement between simulation and theory. More figures to come.\\
To-do list:
\begin{itemize}
\item Assess the sensitivity of algorithm on the ``distribution'' on core tensor. 
\item Understand the bad convergence that arises in simulations.  
\item Evaluate the BIC rank selection accuracy via simulation.
\item Evaluate the MSE for supervised regression.
\item Apply the method to a real dataset. Possible dataset: multi-relational social networks with user attributes; air-flight connection map with flight attributes; or nation data with multilayer political networks (already tried; see earlier note). 
\end{itemize}
\section{Proofs}

\begin{proof}[Proof of Theorem~\ref{thm:main}]
Following the similar argument as in [Wang and Li, 2019], we have $\text{Log-lik}(\tB_\text{true})\leq \text{Log-lik}(\hat \tB_{\text{MLE}})$. By Taylor expansion, 
\begin{equation}\label{eq:taylor}
\FnormSize{}{(\hat \tB_{\text{MLE}}-\tB_{\text{true}})\times_1 \mX_1\times_2 \cdots\times_K \mX_K}^2 \leq C_\alpha\langle \tS,\ (\hat \tB_{\text{MLE}}-\tB_{\text{true}})\times_1 \mX_1\times_2 \cdots\times_K \mX_K \rangle ,\\
\end{equation}
where $\tS\in\mathbb{R}^{d_1\times \cdots \times d_K}$ is a random tensor consisting of i.i.d. bounded random entries. Applying the RIP condition to $(\hat \tB_{\text{MLE}}-\tB_{\text{true}})\in\tP(2\mr,2\alpha)$ in the inequality~\eqref{eq:taylor} yields
\begin{align}
&d (1-\delta_{2\mr,2\alpha})\FnormSize{}{(\hat \tB_{\text{MLE}}-\tB_{\text{true}})}^2\\
\leq &\FnormSize{}{(\hat \tB_{\text{MLE}}-\tB_{\text{true}})\times_1 \mX_1\times_2 \cdots\times_K \mX_K}^2\\
\leq & C_\alpha\times \FnormSize{}{\hat \tB_{\text{MLE}}-\tB_{\text{true}}}\times \sqrt{ d(1+\delta_{2\mr,2\alpha}) {\prod_k r_k\over r_{\max}}\sum_k p_k},
\end{align}
where $d=\prod_k d_k$ and the last line uses the Lemma~\ref{lem}. Therefore,
\[
\FnormSize{}{\hat \tB_{\text{MLE}}-\tB_{\text{true}}}\leq {C_\alpha \over \sqrt{\prod_k d_k} } \sqrt{ {(1+\delta_{2\mr,2\alpha})\over (1-\delta_{2\mr,2\alpha})^2}  {\prod_k r_k \over r_{\max}} \sum_k p_k}.
\]
\end{proof}


\begin{lem}\label{lemma:RIP}
Suppose the matrices $\{\mX_k\}$ satisfy the RIP condition with constant $\delta_{\mr,\alpha}\in(0,1)$. Then the matrices $\{ \tilde \mX^T_k\mX_k \}$ also satisfy the RIP condition with the same RIP constant. 
\end{lem}
\begin{proof}
Let $\mX_k=\mP_k\Delta_k \mQ^T_k$ be the SVD of $\mX_k$, and by Property~\ref{eq:defn}, $\tilde\mX^T_k\mX_k=\mQ\Delta_k\mQ^T\in\mathbb{R}^{p_k\times p_k}$. Note that the F-norm is invariant under orthonormal transformation. Hence,
\begin{align}
\FnormSize{}{\tB\times_1\mX_1\times_2\cdots\times_K\mX_K}&=\FnormSize{}{\tB\times_1(\mP_1\Delta_1\mQ^T_1)\times_2\cdots\times_K(\mP_K\Delta_K\mQ^T_K)}\\
&=\FnormSize{}{\tB\times_1 (\mQ\Delta_1\mQ^T)\times_2\cdots\times_K(\mQ\Delta_K\mQ^T)}\\
&=\FnormSize{}{\tB\times_1 (\tilde \mX_1\mX^T_1)^{1/2}\times_2\cdots\times_K (\tilde \mX_K\mX_K)^{1/2} }.
\end{align}
The proof is complete by invoking the Assumption~\ref{ass:RIP}.
\end{proof}

\begin{lem}\label{lem}
Let $\tB\in\tP(\mr,\alpha)$ be a fixed tensor in the parameter space $\tP(\mr, \alpha)$ and $\tS\in\mathbb{R}^{d_1\times \cdots \times d_K}$ be a random tensor with i.i.d.\ bounded random entries. Denote $d=\prod_k d_k$. Suppose $\{\mX_k\}$ satisfy the RIP condition with RIP constant $\delta_{\mr, \alpha}$. Then, with very high probability,
\[
\langle \tS,\ \tB\times_1\mX_1\times_2\cdots\times_K\mX_K \rangle \leq \FnormSize{}{\tB} \times\sqrt{ d(1+\delta_{\mr,\alpha}) {\prod_{k=1}^Kr_k \over r_{\max}}\sum_{k=1}^K p_k}.
\]
\end{lem}
\begin{proof}
Let $\tilde \mX_k=\mX_k(\mX_k^T\mX_k)^{-1/2}=\mP_k$, where $\mP_k$ consists of left singular vectors of $\mX$. By the definition of inner product, 
\begin{align}
&\langle \tS,\ \tB\times_1\mX_1\times_2\cdots\times_K\mX_K \rangle\\
=& \Big\langle \KeepStyleUnderBrace{\tS\times_1\tilde \mX^T_1\times_2\cdots \times_K\tilde \mX^T_K}_{:=\tE\in\mathbb{R}^{p_1\times \cdots\times p_K}\text{ is a sub-Gaussian(1) tensor by Lemma~\ref{lem:sub}}} ,\ \tB\times_1(\tilde \mX^T_1\mX_1)\times_2 \cdots \times_K (\tilde \mX^T_K\mX_K)\Big \rangle.\\
\leq & \snormSize{}{\tE} \times \nnorm{\tB\times_1(\tilde \mX^T_1\mX_1)\times_2 \cdots \times_K (\tilde \mX^T_K\mX_K) } \\
\leq &    \snormSize{}{\tE} \times \sqrt{\prod_k r_k \over r_{\max}}\times \Fnorm{\tB \times_1 (\tilde \mX^T_1\mX_1)\times_2 \cdots \times_K (\tilde \mX^T_K\mX_K)   }\\
\leq & \sqrt{\prod_k r_k \over r_{\max}}\times  \snormSize{}{\tE}\times \sqrt{d(1+\delta_{\mr,\alpha})} \FnormSize{}{\tB},
%\leq & \sqrt{\prod_k r_k \over r_{\max}}\times  \snormSize{}{\tE}\times  \FnormSize{}{\tB} \times \prod_k \snormSize{}{\tilde \mX^T_k\mX_k}\\
%&\leq \sqrt{\prod_k r_k \over r_{\max}}\times  \snormSize{}{\tE}\times  \FnormSize{}{\tB} \times \prod_k \snormSize{}{\mX_k}\\
%&\leq \sqrt{\prod_k r_k \over r_{\max}}\times  \snormSize{}{\tE}\times  \FnormSize{}{\tB} \times \prod_k \snormSize{}{\mX_k}\\
\end{align}
where the last line comes from the RIP condition of $\{\tilde \mX^T_k\mX_k\}$ by Lemma~\ref{lemma:RIP}. Combining with the fact that $\snormSize{}{\tE}\asymp \tO( \sqrt{\sum_k p_k})$(c.f. Theorem 1 in Tommioka and Suzuki, 2014], we have 
\[
\langle \tS,\ \tB\times_1\mX_1\times_2\cdots\times_K\mX_K \rangle\leq \FnormSize{}{\tB}\times \sqrt{d(1+\delta_{\mr,\alpha}){\prod_k r_k \over r_{\max}}\sum_k p_k}.
\] 
\end{proof}

\begin{lem} \label{lem:sub}
Let $\tS$ be an sG$(\sigma)$ tensor of dimension $(d_1,\ldots,d_K)$ and $\tilde \mX_k\in\mathbb{R}^{d_k\times p_k}$ be column-wise orthogonal matrices. Then $\tE=\tS \times_1 \tilde \mX^T_1\times_2\cdots\times_K \tilde \mX^T_K$ is an sG$(\sigma)$ tensor of dimension $(p_1,\ldots,p_K)$.
\end{lem}

\begin{proof} (Extended from Zhuoyan's note version 4.0)
To show $\tE$ is an sG tensor, it suffices to show that the $\tE(\bmu_1, \bmu_2, \cdots, \bmu_K)\stackrel{\text{def}}{=}\langle\tE, \bmu_1\otimes \cdots\otimes \bmu_K \rangle$ is a sub-Gaussian random variable with parameter $\sigma$, where $\bmu_k\in\mS^{p_k-1}$ for all $k=1,\ldots,K$.

Note that, 
\[
\tE(\bmu_1,\ \cdots,\ \bmu_K )=\tS(\tilde \mX_1\bmu_1,\ \ldots \tilde \mX_K\bmu_K).
\]
Because $\tilde \mX_k\in\mathbb{R}^{d\times p}$ are column-wise orthogonal matrices, so $\vnormSize{}{\tilde \mX_k\bmu_k}=\vnormSize{}{\bmu_k}=1$. By definition of sub-Gaussian tensor, $\tS(\tilde \mX_1\bmu_1,\ \ldots \tilde \mX_K\bmu_K)$ is a sub-Gaussian random variable with parameter $\sigma$, so is the $\tE(\bmu_1,\ldots,\bmu_K)$. 
\end{proof}

\end{document}