\documentclass[11pt]{article}
\usepackage{lscape}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{float}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{bm}
\usepackage{gensymb}
\allowdisplaybreaks[4]
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{setspace}
\usepackage{siunitx}
\usepackage{enumitem}
\usepackage{dsfont}

\usepackage{graphics}
\usepackage[noend]{algorithmic}
\usepackage[utf8x]{inputenc}
\usepackage{bm}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    citecolor = blue,
    linkcolor=blue,
    filecolor=magenta,           
    urlcolor=cyan,
}


\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{pro}{Property}
\newtheorem{cor}{Corollary}
\newtheorem{ass}{Assumption}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{exmp}{Example}
\newtheorem{rmk}{Remark}



\usepackage[labelfont=bf]{caption}

\setcounter{table}{1}
 % \usepackage[labelformat=empty]{ caption}
\usepackage{multirow}
\usepackage{tabularx}

\def\fixme#1#2{\textbf{[FIXME (#1): #2]}}

 

\newcommand*{\KeepStyleUnderBrace}[1]{%f
  \mathop{%
    \mathchoice
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}
% \usepackage[initials]{amsrefs}
%\usepackage{amsaddr}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs=true}


\usepackage{hyperref}
\hypersetup{colorlinks=true}
\usepackage[parfill]{parskip}
\usepackage{bm}
\onehalfspacing
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%             Math Symbols
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%               Bold Math
\input macros.tex
\def\refer#1{\emph{\color{blue}#1}}

\usepackage{sectsty}
\sectionfont{\fontsize{12}{15}\selectfont}

\newcommand*{\QEDB}{\hfill\ensuremath{\square}}%


%\author{%
%Yuchen Zeng \\
%University of Wisconsin -- Madison\\
 %\texttt{yzeng58@wisc.edu} \\
%\And
%Miaoyan Wang \\
%University of Wisconsin -- Madison\\
%\texttt{miaoyan.wang@wisc.edu} \\
%}

\begin{document}
\begin{center}
<<<<<<< HEAD:note/team_jiaxin_zhuoyan/note_0821.tex
{\bf \large Paper Sketch for AISTATS\\
Tentative title: ``Binary tensor regression with multi-mode features''}\\
first draft on 08/13, updated on 08/21
=======
{\bf \large Statistical analysis of low-rank binary tensor regression}\\
Miaoyan Wang\quad 08/13/2019
>>>>>>> e196d6bc53c6b52dd30cf0a2a6b6f020027d1dab:nips/submission/source/note.tex
\end{center}

\section{Preliminaries}
\begin{defn}\label{eq:defn}
Let $\mX_k\in\mathbb{R}^{p_k\times d_k}$ be a rank-$s_k$ matrix. The SVD of $\mX_k$ can be expressed as $\mX_k=\mP_k\Delta_k \mQ^T_k$, where $\mP_k\in\mathbb{R}^{p_k\times s_k}$ and $\mQ_k\in\mathbb{R}^{d_k\times s_k}$ consist of, respectively, the left and right singular vectors, and $\Delta_k\in\mathbb{R}^{s_k\times s_k}$ is the diagonal matrix consisting of non-zero singular values. We introduce the following short-hand notions:
\begin{enumerate}
\item $(\mX_k\mX^T_k)^{1/2}=\mP_k\Delta_k\in\mathbb{R}^{p_k\times s_k}$,
\item $(\mX_k\mX^T_k)^{-1/2}=\Delta^{-1}_k\mP^T_k\in\mathbb{R}^{s_k\times p_k}$.
\end{enumerate}

Note that $(\mX_k\mX^T_k)^{1/2}$ are  Moore-Penrose inverse of $(\mX_k\mX^T_k)^{-1/2}$ and $\left((\mX_k\mX^T_k)^{-1/2}\right)^T=\mP_k\Delta^{-1}_k\in\mathbb{R}^{p_k\times s_k}$.
\end{defn}

We use lower-case letters $(a, b, \ldots)$ for scalars and vectors, upper-case boldface letters $(\mA, \mB, \ldots)$ for matrices, and calligraphy letter $(\tA,\tB,\ldots)$ for tensors of order 3 or greater. Let $\tY \in \mathbb{R}^{d_1\times \cdots \times d_K}$ denote an order-$K$ $(d_1,\ldots,d_K)$-dimensional tensor. We say that an event $A$ occurs ``with very high probability'' if $\mathbb{P}(A)$ tends to 1 faster than any polynomial of $d_{\min}=\min\{d_1, . . . , d_K\}$.



\section{Results}
Suppose we observe an order-$K$ binary tensor $\tY\in\{0,1\}^{d_1\times \cdots \times d_K}$, along with a set of covariate matrices $\mX_k\in\mathbb{R}^{p_k\times d_k}$ for $k=1,\ldots,K$. Consider a tensor regression model:
\begin{equation}\label{eq:model}
\text{logit}(\mathbb{E}(\tY))=\tB\times_1\mX_1\times_2\cdots\times_K \mX_K,
\end{equation}
where $\tB\in \mathbb{R}^{p_1\times \cdots \times p_K}$ is a coefficient tensor of interest. Furthermore, the tensor $\tB$ is assumed to (i) be entrywise bounded,  and (ii) admit a low-rank Tucker decomposition; that is, $\text{rank}(\tB)=\mr\equiv(r_1,\ldots,r_K)^T$, where $r_k\leq p_k\leq d_k$. The parameter space we consider is
\[
\tP=\tP(\mr,\alpha)=\{ \tB \in \mathbb{R}^{p_1\times \cdots \times p_K}\colon \text{rank}(\tB)\leq \mr, \ \text{and} \ \mnormSize{}{\tB}\leq \alpha\}.
\]

In the following analysis, we assume both the multilinear rank $\mr$ and entrywise bound $\alpha$ are known. The adaptation of unknown rank will be addressed in the next note. 

\begin{rmk}
Model~\eqref{eq:model} incorporates the following examples as special cases:

(1) {\bf Binary tensor decomposition}. In the absence of side information, set $\mX=\mI_k$ to be identity matrix and $p_k=d_k$ for $k=1,\ldots,K$. Then the model~\eqref{eq:model} reduces to unsupervised binary tensor decomposition.

(2) {\bf Network link prediction model}. Suppose $K=2$ and $\mX_1=\mX_2$. Then the model~\eqref{eq:model} reduced to the matrix logistic model [Baldin and Berthet, 2018] that is commonly used in the network analysis:
\[
\text{logit}(\mathbb{E}(\mY))=\mX^T\mB\mX,\quad \text{where}\quad \text{rank}(\mB)\leq r. 
\]

(3) {\bf Semi-supervised decomposition}. Suppose the covariate information is available only for a subset of modes. Without loss of generality, suppose the covariates $\mX_k\neq \mI$ are available in modes $1,\ldots,L$, where $L< K$. Then the model~\eqref{eq:model} reduces to a semi-supervised decomposition model:
\[
\text{logit}(\mathbb{E}(\tY))=\KeepStyleUnderBrace{\tB}_{\text{$\in\mathbb{R}^{{\color{red}p_1\times \cdots\times p_L}\times d_{L+1}\times \cdots \times  d_K}$}} \times_1 \KeepStyleUnderBrace{\mX_1}_{\text{$\in \mathbb{R}^{{\color{red}p_1}\times d_1}$}}\times_2\cdots\times_L\KeepStyleUnderBrace{\mX_L}_{\text{$\in \mathbb{R}^{{\color{red}p_L}\times d_L}$}}.
\] 
\end{rmk}

For parsimony, we do not distinguish modes with available side information from those without side information. We focus on the general tensor regression model~\eqref{eq:model} with mild assumption on $\{\mX_k\}$. Specifically, the covariates $\{\mX_k\}$ are assumed to satisfy the following restricted isometry property (RIP) assumption. 

\begin{ass}[Restricted Isometry Property]\label{ass:RIP}
Let $d=\prod_k d_k$. The covariates $\{\mX_k\}$ are called to satisfy the RIP condition if there exists a positive constant $\delta_{\mr,\alpha}\in(0,1)$ such that 
\[
d(1-\delta_{\mr,\alpha})\Fnorm{\tB}^2\leq \Fnorm{\tB \times_1\mX_1\times_2\cdots\times_K\mX_K}^2\leq d(1+\delta_{\mr,\alpha})\Fnorm{\tB}^2,
\]
holds for all tensors $\tB\in \tP(\mr,\alpha)$ in the parameter space. 
\end{ass}
\begin{rmk}
The RIP assumption requires the covariates at each of the modes are nearly orthonormal, at least when restricted to the desired parameter space. The RIP condition is a relatively mild assumption on the covariates. In particular, it incorporates the fixed design and Gaussian random design as special cases. 
\end{rmk}

<<<<<<< HEAD:note/team_jiaxin_zhuoyan/note_0821.tex
\begin{exmp} [Gaussian Random design] Suppose $\mX_k \in\mathbb{R}^{d_k\times p_k}$ are random matrix with i.i.d.\ standard Gaussian entries, where ${p_k\over d_k}=\lambda_k\in(0,1)$ for all $k=1,\ldots,K$. Then, with very high probability, $\{\mX_k\}$ satisfy the RIP condition. In particular, the RIP constant can be chosen as
\[
\delta_{\mr,\alpha}=\max\left\{ -1+\prod_k (1+\lambda_k),\ 1-\prod_k (1-\lambda_k)\right\} \in (0,1)}.
\]
\end{exmp}

\begin{exmp}[Fixed design] Suppose $\mX_k \in\mathbb{R}^{d_k\times p_k}$ are deterministic matrices whose singular values satisfy
\[
{ \sigma_{\max}(\mX_k)\over \sigma_{\min}(\mX_k)} \in[1,2),\quad \text{for all }k=1,\ldots,K.
\]
Then, $\{\mX_k\}$ (upon proper rescaling) satisfy the RIP condition.
\end{exmp}

=======
>>>>>>> e196d6bc53c6b52dd30cf0a2a6b6f020027d1dab:nips/submission/source/note.tex
\begin{thm} [Main Results]
\label{thm:main}
Let $\hat \tB_{\text{MLE}}$ be the restricted maximum likelihood estimate of the model~\eqref{eq:model}; i.e.,
\[
\hat \tB_{\text{MLE}}=\argmin_{\tB\in\tP(\mr,\alpha)} \text{Log-lik}\ (\tB; \tY, \{\mX_k\}),\quad \text{where}\ \{\mX_k\}\ \text{satisfy the RIP condition}.
\]
Then, with very high probability,
\[
\Fnorm{\hat \tB_{\text{MLE}}-\tB_{\text{true}}}\leq {C_\alpha\over d} \sqrt{ {(1+\delta_{2\mr,2\alpha})\over (1-\delta_{2\mr,2\alpha})^2}{\prod^K_{k=1} r_k\over r_{\max}} \sum^K_{k=1} p_k},
\]
where $C_\alpha>0$ is a constant that does not depend on the dimension or rank. 
\end{thm}

\section{Experiments}
{\bf Algorithm sketch}. Please refer to earlier notes for algorithm. \\
{\bf Rank selection}: We propose to use Bayesian information criterion (BIC) and choose the rank that minimizes BIC; i.e.
\[
\hat \mr=\argmin_{\mr=(r_1,\ldots,r_K)'} BIC(\mr)=\argmin_{\mr=(r_1,\ldots,r_K)'}\left[-2\text{Log-lik}(\hat \Theta)+p_e(r_1,\ldots,r_K)\log \left(\prod_k d_k\right) \right],
\]
where $p_e(r_1,\ldots,r_K)\stackrel{\text{def}}{=}\sum_k (d_k-1)r_k+\prod_k r_k$ is the effective number of parameters in the model. 

{\bf Performance.} There are several issues summarized in the note ``Summary\_of\_last\_semester\_
and\_summer\_plan.pdf''. The main issues are
\begin{itemize}
\item The choice of distribution to generate the core tensor. Bad estimation when the core is generated i.i.d.\ $N(0,1)$, $N(10,1)$, $\text{Unif}[0,1]$ or $\text{Unif}[0,10]$.
\item When to stop? ``It is reported that when we run more iterations, the result sometime seems not better or even worse.''
\item Unbounded estimation. ``Sometimes, when we check the real scale of U?, there may some entries with extremely large values''.
\end{itemize}
We have proposed to impose an infinity-norm constraint to the optimization. Which of the following leads to the best result?
\begin{enumerate}
\item Update the core tensor to the feasible region via conjugate gradient solver. (good MSE, but haven not checked its performance on ``bad'' distributions. )
\item Update the core by global downscaling.
\item Update the factor matrixes by backward linear search (computationally slow).
\end{enumerate}


MSE exhibits good agreement between simulation and theory. More figures to come.\\
To-do list:
\begin{itemize}
\item Assess the sensitivity of algorithm on the ``distribution'' on core tensor. 
\item Understand the bad convergence that arises in simulations.  
\item Evaluate the BIC rank selection accuracy via simulation.
\item Evaluate the MSE for supervised regression.
\item Apply the method to a real dataset. Possible dataset: multi-relational social networks with user attributes; air-flight connection map with flight attributes; or nation data with multilayer political networks (already tried; see earlier note). 
\end{itemize}
\section{Proofs}

\begin{proof}[Proof of Theorem~\ref{thm:main}]
Following the similar argument as in [Wang and Li, 2019], we have $\text{Log-lik}(\tB_\text{true})\leq \text{Log-lik}(\hat \tB_{\text{MLE}})$. By Taylor expansion, 
\begin{equation}\label{eq:taylor}
\FnormSize{}{(\hat \tB_{\text{MLE}}-\tB_{\text{true}})\times_1 \mX_1\times_2 \cdots\times_K \mX_K}^2 \leq C_\alpha\langle \tS,\ (\hat \tB_{\text{MLE}}-\tB_{\text{true}})\times_1 \mX_1\times_2 \cdots\times_K \mX_K \rangle ,\\
\end{equation}
where $\tS\in\mathbb{R}^{d_1\times \cdots \times d_K}$ is a random tensor consisting of i.i.d. bounded random entries. Applying the RIP condition to $(\hat \tB_{\text{MLE}}-\tB_{\text{true}})\in\tP(2\mr,2\alpha)$ in the inequality~\eqref{eq:taylor} yields
\begin{align}
&(1-\delta_{2\mr,2\alpha})\FnormSize{}{(\hat \tB_{\text{MLE}}-\tB_{\text{true}})}^2\\
\leq &\FnormSize{}{(\hat \tB_{\text{MLE}}-\tB_{\text{true}})\times_1 \mX_1\times_2 \cdots\times_K \mX_K}^2\\
\leq & C_\alpha\times \FnormSize{}{\hat \tB_{\text{MLE}}-\tB_{\text{true}}}\times \sqrt{ (1+\delta_{2\mr,2\alpha}) {\prod_k r_k\over r_{\max}}\sum_k p_k},
\end{align}
where the last line uses the Lemma~\ref{lem}. Therefore,
\[
\FnormSize{}{\hat \tB_{\text{MLE}}-\tB_{\text{true}}}\leq C_\alpha \sqrt{ {(1+\delta_{2\mr,2\alpha})\over(1-\delta_{2\mr,2\alpha})^2}  {\prod_k r_k \over r_{\max}} \sum_k p_k}.
\]
\end{proof}


\begin{lem}\label{lemma:RIP}
Suppose the matrices $\{\mX_k\}$ satisfy the RIP condition with constant $\delta_{\mr,\alpha}\in(0,1)$. Then the matrices $\{ \left(\mX_k\mX^T_k\right)^{1/2}\}$ also satisfy the RIP condition with the same RIP constant. 
\end{lem}
\begin{proof}
Let $\mX_k=\mP_k\Delta_k \mQ^T_k$ be the SVD of $\mX_k$, and by definition~\ref{eq:defn}, $(\mX_k\mX^T_k)^{1/2}=\mP_k\Delta_k\in\mathbb{R}^{p_k\times s_k}$. Note that the F-norm is invariant under orthonormal transformation. Hence,
\begin{align}
\FnormSize{}{\tB\times_1\mX_1\times_2\cdots\times_K\mX_K}&=\FnormSize{}{\tB\times_1(\mP_1\Delta_1\mQ^T_1)\times_2\cdots\times_K(\mP_K\Delta_K\mQ^T_K)}\\
&=\FnormSize{}{\tB\times_1 (\mP_1\Delta_1)\times_2\cdots\times_K(\mP_K\Delta_K)}\\
&=\FnormSize{}{\tB\times_1 (\mX_1\mX^T_1)^{1/2}\times_2\cdots\times_K (\mX_K\mX^T_K)^{1/2} }.
\end{align}
The proof is complete by revoking the Assumption~\ref{ass:RIP}.
\end{proof}

\begin{lem}\label{lem}
Let $\tB\in\tP(\mr,\alpha)$ be a fixed tensor in the parameter space $\tP(\mr, \alpha)$ and $\tS\in\mathbb{R}^{d_1\times \cdots \times d_K}$ be a random tensor with i.i.d.\ bounded random entries. Suppose $\{\mX_k\}$ satisfy the RIP condition with RIP constant $\delta_{\mr, \alpha}$. Then, with very high probability,
\[
\langle \tS,\ \tB\times_1\mX_1\times_2\cdots\times_K\mX_K \rangle \leq \FnormSize{}{\tB} \times\sqrt{ (1+\delta_{\mr,\alpha}) {\prod_{k=1}^Kr_k \over r_{\max}}\sum_{k=1}^K p_k}.
\]
\end{lem}
\begin{proof}
By the definition of inner product, 
\begin{align}
&\langle \tS,\ \tB\times_1\mX_1\times_2\cdots\times_K\mX_K \rangle\\
=& \Big\langle \KeepStyleUnderBrace{\tS\times_1\left[\mX_1^T\left((\mX_1\mX^T_1)^{-1/2}\right)^T\right]\times_2\cdots \times_K\left[\mX_K^T\left((\mX_K\mX^T_K)^{-1/2}\right)^T\right]}_{:=\tE\in\mathbb{R}^{p_1\times \cdots\times p_K}\text{ is a random tensor with i.i.d. bounded entries}} ,\ \tB\times_1\left[(\mX_1\mX^T_1)^{1/2}\right]\times_2\cdots\times_K \left[(\mX_K\mX^T_K)^{1/2}\right]\Big \rangle\\
\leq & \snormSize{}{\tE} \times \nnorm{\tB \times_1\left[(\mX_1\mX^T_1)^{1/2}\right]\times_2\cdots\times_K \left[(\mX_K\mX^T_K)^{1/2}\right] } \\
\leq &    \snormSize{}{\tE} \times \sqrt{\prod_k r_k \over r_{\max}}\times \Fnorm{\tB \times_1\left[(\mX_1\mX^T_1)^{1/2}\right]\times_2\cdots\times_K \left[(\mX_K\mX^T_K)^{1/2}\right] }\\
\leq & \sqrt{\prod_k r_k \over r_{\max}}\times  \snormSize{}{\tE}\times \sqrt{1+\delta_{\mr,\alpha}} \FnormSize{}{\tB},
\end{align}
where the last line comes from the RIP condition of $\{(\mX_k\mX_k)^{1/2}\}$ by Lemma~\ref{lemma:RIP}. Combining with the fact that $\snormSize{}{\tE}\asymp \tO( \sqrt{\sum_k p_k})$ (c.f. Theorem 1 in Tommioka and Suzuki, 2014], we have 
\[
\langle \tS,\ \tB\times_1\mX_1\times_2\cdots\times_K\mX_K \rangle\leq \FnormSize{}{\tB}\times \sqrt{(1+\delta_{\mr,\alpha}){\prod_k r_k \over r_{\max}}\sum_k p_k}.
\] 
\end{proof}


\end{document}