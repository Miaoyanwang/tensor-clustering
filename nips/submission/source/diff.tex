\documentclass{article}
%DIF LATEXDIFF DIFFERENCE FILE
%DIF DEL tensor_block_model.tex        Wed Aug 21 15:20:16 2019
%DIF ADD tensor_block_model_0915.tex   Sun Sep 15 23:05:40 2019

% if you need to pass options to natbib, use, e.g.:

% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
 % \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
%DIF 15c15
%DIF <  \usepackage[nonatbib]{neurips_2019}
%DIF -------
 \usepackage[final]{neurips_2019} %DIF > 
%DIF -------
% to avoid loading the natbib package, add option nonatbib:
%DIF 17c17
%DIF <  % \usepackage[nonatbib]{neurips_2019}
%DIF -------
%\usepackage[nonatbib]{neurips_2019} %DIF > 
%DIF -------
\usepackage{graphicx}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{bm}
\usepackage{subfig}
\usepackage[english]{babel}
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{appendix}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{ass}{Assumption}
\newtheorem{defn}{Definition}
\newtheorem{exam}{Example}
\newtheorem{proof}{Proof}
\input macros.tex
\usepackage{dsfont}

\usepackage{multirow}
\usepackage{algpseudocode,algorithm}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\algorithmicoutput{\textbf{Output:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\algnewcommand\OUTPUT{\item[\algorithmicoutput]}

%DIF 51a51-54
\DeclareMathOperator*{\minimize}{minimize} %DIF > 
 %DIF > 
 %DIF > 
 %DIF > 
%DIF -------
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\newcommand*{\KeepStyleUnderBrace}[1]{%f
  \mathop{%
    \mathchoice
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}
%DIF 62d66
%DIF < 
%DIF -------
\usepackage{xr}
\externaldocument{tensor_block_model_supp}
%DIF 65a68
 %DIF > 
%DIF -------


\title{Multiway clustering via tensor block models}


\author{%
Yuchen Zeng \\
 University of Wisconsin -- Madison\\
 \texttt{yzeng58@wisc.edu} \\
\And
Miaoyan Wang \\
 University of Wisconsin -- Madison\\
\texttt{miaoyan.wang@wisc.edu} \\
}
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFaddtex}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
\providecommand{\DIFdeltex}[1]{{\protect\color{red}\sout{#1}}}                      %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF HYPERREF PREAMBLE %DIF PREAMBLE
\providecommand{\DIFadd}[1]{\texorpdfstring{\DIFaddtex{#1}}{#1}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{\texorpdfstring{\DIFdeltex{#1}}{}} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}

\maketitle

\vspace{-.3cm}
\begin{abstract}
\vspace{-.2cm}
\DIFaddbegin 

\DIFaddend We consider the problem of identifying multiway block structure from a large noisy tensor. Such problems arise frequently in applications such as genomics, recommendation system, topic modeling, and sensor network localization. We propose a tensor block model, develop a unified least-square estimation, and obtain the theoretical accuracy guarantees for multiway clustering. The statistical convergence of the estimator is established, and we show that the associated clustering procedure achieves partition consistency. A sparse regularization is further developed for identifying important blocks with elevated means. The proposal handles a broad range of data types, including binary, continuous, and hybrid observations. Through simulation and application to two real datasets, we demonstrate the outperformance of our approach over previous methods. 
\end{abstract}

\section{Introduction}
\vspace{-.2cm}
Higher-order tensors have recently attracted increased attention in data-intensive fields such as neuroscience~\cite{cong2015tensor,zhou2013tensor}, social networks~\cite{nickel2011three,socher2013reasoning}, computer vision~\cite{tang2013tensor,liu2013tensor}, and genomics~\cite{wang2017three,hore2016tensor}. In many applications, the data tensors are often expected to have underlying block structure. One example is multi-tissue expression data~\cite{wang2017three}, in which genome-wide expression profiles are collected from different tissues in a number of individuals. There may be groups of genes similarly expressed in subsets of tissues and individuals; mathematically, this implies an underlying three-way block structure in the data tensor. In a different context, block structure may emerge in a binary-valued tensor. Examples include multilayer network data~\cite{nickel2011three}, with the nodes representing the individuals and the layers representing the multiple types of relations. Here a planted block represents a community of individuals that are highly connected within a class of relationships. 

\begin{figure}[h!]
\vspace{-.2cm}
\centering
\includegraphics[width=.8\textwidth]{figures/demo.pdf}
\caption{\small Examples of tensor block model (TBM). (a) Our TBM method is used for multiway clustering and for revealing the underlying checkerbox structure in a noisy tensor. (b) The sparse TBM method is used for detecting sub-tensors of elevated means. }
\label{fig:1}
\vspace{-.2cm}
\end{figure}

This paper presents a new method and the associated theory for tensors with block structure. We develop a unified least-square estimation procedure for identifying multiway block structure. The proposal applies to a broad range of data types, including binary, continuous, and hybrid observations. We establish a high-probability error bound for the resulting estimator, and show that the procedure enjoys consistency guarantees on the block structure recovery as the dimension of the data tensor grows. Furthermore, we develop a sparse extension of the tensor block model for block selections. Figure~\ref{fig:1} shows two immediate examples of our method. When the data tensor possesses a checkerbox pattern modulo some unknown reordering of entries, our method amounts to multiway clustering that simultaneously clusters each mode of the tensor (Figure~\ref{fig:1}a). When the data tensor has no full checkerbox structure but contains a small numbers of sub-tensors of elevated means, we develop a sparse version of our method to detect these sub-tensors of interest (Figure~\ref{fig:1}b). 

{\bf Related work.} Our work is closely related to, but also clearly distinctive from, the low-rank tensor decomposition. A number of methods have been developed for low-rank tensor estimation, including CANDECOMP/PARAFAC (CP) decomposition~\cite{hitchcock1927expression} and Tucker decomposition~\cite{tucker1966some}. The CP model decomposes a tensor into a sum of rank-1 tensors, whereas Tucker model decomposes a tensor into a core tensor multiplied by orthogonal matrices in each mode. In this paper we investigate an alternative block structure assumption, which has yet to be studied for higher-order tensors. Note that a block structure automatically implies low-rankness. However, as we will show in Section~\ref{sec:theory}, a direct application of low rank estimation to the current setting will result in an inferior estimator. Therefore, a full exploitation of the block structure is necessary; this is the focus of the current paper. 

Our work is also connected to biclustering and its higher-order extensions~\cite{tan2014sparse,kolda2008scalable,wang2015multi}. Existing multiway clustering methods~\cite{kolda2008scalable,wang2015multi,hore2016tensor} typically take a two-step procedure, by first estimating a low-dimension representation of the data tensor and then applying clustering algorithms to the tensor factors. In contrast, our tensor block model takes a single shot to perform estimation and clustering simultaneously. This approach achieves a higher accuracy and an improved interpretability. Moreover, earlier solutions to multiway clustering~\cite{jegelka2009approximation,kolda2008scalable} focus on the algorithm effectiveness, leaving the statistical optimality of the estimators unaddressed. Very recently, Chi et al~\cite{chi2018provable} provides an attempt to study the statistical properties of the tensor block model. We will show that our estimator obtains a faster convergence rate than theirs, and the power is further boosted with a sparse regularity. 

\vspace{-.1cm}
\section{Preliminaries}
\vspace{-.1cm}
We begin by reviewing a few basic factors about tensors~\cite{kolda2009tensor}. We use $\tY=\entry{y_{i_1,\ldots,i_K}}\in \mathbb{R}^{d_1\times \cdots\times d_K}$ to denote an order-$K$ $(d_1,\ldots,d_K)$-dimensional tensor. The multilinear multiplication of a tensor $\tY\in\mathbb{R}^{d_1\times \cdots\times d_K}$ by matrices \DIFdelbegin \DIFdel{$\mM_k=\entry{m_{i_k,j_k}^{(k)}}\in\mathbb{R}^{d_k\times s_k}$ }\DIFdelend \DIFaddbegin \DIFadd{$\mM_k=\entry{m_{i_k,j_k}^{(k)}}\in\mathbb{R}^{s_k\times d_k}$ }\DIFaddend is defined as
\[
\tY \times_1\mM_1\ldots \times_K \mM_K=\entry{\sum_{i_1,\ldots,i_K}y_{i_1,\ldots,i_K}m_{i_1,j_1}^{(1)}\ldots m_{i_K,j_K}^{(K)}},
\]
which results in an order-$K$ tensor $(s_1,\ldots,s_K)$-dimensional tensor. For any two tensors $\tY=\entry{y_{i_1,\ldots,i_K}}$, $\tY'=\entry{y'_{i_1,\ldots,i_K}}$ of identical order and dimensions, their inner product is defined as $\langle \tY, \tY'\rangle =\sum_{i_1,\ldots,i_K}y_{i_1,\ldots,i_K}y'_{i_1,\ldots,i_K}$. The Frobenius norm of tensor $\tY$ is defined as $\FnormSize{}{\tY}=\langle \tY, \tY \rangle^{1/2}$; it is the Euclidean norm of $\tY$ regarded as an $\prod_k d_k$-dimensional vector. A fiber of $\tY$ is an order-($K$-$1$) sub-tensor of $\tY$ obtained by holding the index in one mode fixed while letting other indices vary. 
%For example, $\entry{y_{i_1,\ldots}}\in\mathbb{R}^{d_2\times \cdots\times d_K}$ represents an order-($K$-$1$) fiber of $\tY$, for some fixed $1\leq i_1\leq d_1$.

A clustering of $d$ objects is a partition of the index set $[d]:=\{1,2,\ldots,d\}$ into $R$ disjoint non-empty subsets. We refer to the number of clusters, $R$, as the clustering size. Equivalently, the clustering (or partition) can be represented using the ``membership matrix''. A membership matrix $\mM\in\mathbb{R}^{R\times d}$ is an incidence matrix whose $(i,j)$-entry is 1 if and only if the element $j$ belongs to the cluster $i$, and 0 otherwise. Throughout the paper, we will use the terms ``clustering'', ``partition'', and ``membership matrix'' exchangeably. For a higher-order tensor, the concept of index partition applies to each of the modes. A block is a sub-tensor induced by the index partitions along each of the $K$ modes. We use the term ``cluster'' to refer to the marginal partition on mode $k$, and reserve the term ``block'' for the multiway partition of the tensor. 

%We use lower-case letters ($a,b,c,\ldots$) for scalars and vectors, upper-case boldface letters ($\mA,\mB,\mC,\ldots$) for matrices, and calligraphy letter ($\tA, \tB, \tC,\ldots$) for tensors of order 3 or greater. 
%We say that an event $A$ occurs ``with high probability'' if $\mathbb{P}(A)$ tends to 1 as the dimension $d_{\min}=\min\{d_1,\ldots,d_k\}$ tends to infinity. We say that $A$ occurs ``with very high probability'' if $\mathbb{P}(A)$ tends to 1 faster than any polynomial of $d_{\min}$. 


\section{Tensor block model}
\vspace{-.1cm}
Let $\tY=\entry{y_{i_1,\ldots,i_K}}\in\mathbb{R}^{d_1\times \cdots\times d_K}$ denote an order-$K$, $(d_1,\ldots,d_K)$-dimensional data tensor. The main assumption of tensor block model (TBM) is that the observed data tensor $\tY$ is a noisy realization of an underlying tensor that exhibits a checkerbox structure (see Figure~\ref{fig:1}a). Specifically, suppose that the $k$-th mode of the tensor consists of $R_k$ clusters. If the tensor entry $y_{i_1,\ldots,i_K}$ belongs to the block determined by the $r_k$th cluster in the mode $k$ for $r_k\in[R_k]$, then we assume that 
\begin{equation}\label{eq:model}
y_{i_1,\ldots,i_K}=c_{r_1,\ldots,r_K}+\varepsilon_{i_1,\ldots,i_K},\quad \text{for }(i_1,\ldots,i_K)\in[d_1]\times\cdots\times [d_K],
\end{equation}
where $c_{r_1,\ldots,r_K}$ is the mean of the tensor block indexed by $(r_1,\ldots,r_K)$,  and $\varepsilon_{i_1,\ldots,i_K}$'s are independent, mean-zero noise terms to be specified later. Our goal is to (i) find the clustering along each of the modes, and (ii) estimate the block means $\{c_{r_1,\ldots,r_K}\}$, such that a corresponding blockwise-constant checkerbox structure emerges in the data tensor. 

The tensor block model~\eqref{eq:model} falls into a general class of non-overlapping, constant-mean clustering models~\cite{madeira2004biclustering}, in that each tensor entry belongs to exactly one block with a common mean. The TBM can be equivalently expressed as a special tensor Tucker model,
\begin{equation}\label{eq:Tucker}
\tY=\tC\times_1\mM_1\times_2\cdots \times_K \mM_K+\tE,
\end{equation}
where $\tC\in\mathbb{R}^{R_1\times\cdots\times R_K}$ is a core tensor consisting of block means, \DIFdelbegin \DIFdel{$\mM_k \in\{0,1\}^{R_k\times d_k}$ }\DIFdelend \DIFaddbegin \DIFadd{$\mM_k \in\{0,1\}^{d_k\times R_k}$ }\DIFaddend is a membership matrix indicating the block allocations along mode $k$ for $k\in[K]$, and $\tE=\entry{\varepsilon_{i_1,\ldots,i_K}}$ is the noise tensor. %The distinction between our model~\eqref{eq:Tucker} and a classical Tucker model is that we require the factors $\mM_k$ to be membership matrices. 
We view the TBM~\eqref{eq:Tucker} as a super-sparse Tucker model, in the sense that the each column of $\mM_k$ consists of one copy of 1's and massive 0's. 

We make a general assumption on the noise tensor $\tE$. The noise terms $\varepsilon_{i_1,\ldots,i_K}$'s are assumed to be independent, mean-zero $\sigma$-subgaussian, where $\sigma>0$ is the subgaussianity parameter. More precisely, 
\begin{equation}\label{eq:noise}
\mathbb{E}e^{\lambda \varepsilon_{i_1,\ldots,i_K}}\leq e^{\lambda^2\sigma^2/2},\quad \text{for all } (i_1,\ldots,i_K)\in[d_1]\times\cdots\times[d_K] \ \text{and all}\ \lambda\in\mathbb{R}.
\end{equation}
Th assumption~\eqref{eq:noise} incorporates common situations such as Gaussian noise, Bernoulli noise, and noise with bounded support. In particular, we consider two important examples of the TBM:
\vspace{-.1cm}
\begin{exam}[Gaussian tensor block model]
Let $\tY$ be a continuous-valued tensor. The Gaussian tensor block model (GTBM) $y_{i_1,\ldots,i_K}  \sim_{\text{i.i.d.}} N(c_{r_1,\ldots,r_K},\sigma^2)$ is a special case of model~\eqref{eq:model}, with the subgaussianity parameter $\sigma$ equal to the error variance. The GTBM serves as the foundation for many tensor clustering algorithms~\cite{jegelka2009approximation,wang2017three,chi2018provable}. 
\end{exam}
\vspace{-.1cm}
\begin{exam}[Stochastic tensor block model]
Let $\tY$ be a binary-valued tensor. The stochastic tensor block model (STBM) $y_{i_1,\ldots,i_K}  \sim_{\text{i.i.d.}} \text{Bernoulli}(c_{r_1,\ldots,r_K})$ is a special case of model~\eqref{eq:model}, with the subgaussianity parameter $\sigma$ equal to ${1\over 4}$. The STBM can be viewed as an extension, to higher-order tensors, of the popular stochastic block model~\cite{abbe2017community,gao2018minimax} for matrix-based network analysis.
\end{exam}
\vspace{-.1cm}
More generally, our model also applied to hybrid error distributions, in which different types of distribution are allowed for different portions of the tensor. This scenario may happen, for example, when the data tensor $\tY$ represents concatenated measurements from multiple data sources. 

Before we discuss the estimation, we present the identifiability of the TBM. 
%$\mM_k$'s and $\tC$ from model~\eqref{eq:Tucker}. The following irreducible assumption is necessary for the tensor block model to be identifiable. 
\vspace{-.1cm}
\begin{ass}[Irreducible core]\label{ass:core}
The core tensor $\tC$ is called irreducible if it cannot be written as a block tensor with the number of mode-$k$ clusters smaller than $R_k$, for any $k\in[K]$. 
\end{ass}
\vspace{-.1cm}
In the matrix case $(K=2)$, the irreducibility is equivalent to saying that $\tC$ has no two identical rows and no two identical columns. In the higher-order case, the assumption requires that none of order-($K$-$1$) fibers of $\tC$ are identical. Note that irreducibility is a weaker assumption than full-rankness. 
\vspace{-.4cm}
\begin{prop}[Identifiability]\label{prop:factors}
Consider a Gaussian or Bernoulli TBM~\eqref{eq:model}. Under Assumption~\ref{ass:core}, the factor matrices $\mM_k$'s are identifiable up to permutations of cluster labels. 
\end{prop}
\vspace{-.1cm}
The identifiability property for the TBM outperforms that for the classical factor model~\cite{darton1980rotation,abdi2003factor}. In the Tucker~\cite{zhang2018tensor,kolda2009tensor} and many other factor analyses~\cite{darton1980rotation,abdi2003factor}, the factors are identifiable only up to orthogonal rotations. Those models recover only the (column) space spanned by $\mM_k$, but not the individual factors. In contrast, our model does not suffer from rotational invariance, and as we show in Section~\ref{sec:theory}, every individual factor is consistently estimated in high dimensions. This brings a benefit to the interpretation of factors in the tensor block model.  

We propose a least-square approach for estimating the TBM. Let $\Theta=\tC\times_1\mM_1\times_2\cdots\times_K\mM_K$ denote the mean signal tensor with block structure. The mean tensor is assumed to belong to the following parameter space
\begin{align}\label{eq:space}
\tP_{R_1,\ldots,R_K}=&\ \big\{ \Theta\in\mathbb{R}^{d_1\times \cdots \times d_K}\colon \Theta=\tC\times_1\mM_1\times_2\cdots\times_K\mM_K, \text{with some}\notag \\
 &\quad \text{membership matrices $\mM_k$'s and a core tensor $\tC\in\mathbb{R}^{R_1\times \cdots \times R_K}$}\big\}\notag .
\end{align}
In the following theoretical analysis, we assume the clustering size $\mR=(R_1,\ldots,R_K)$ is known and simply write $\tP$ for short. The adaptation of unknown $\mR$ will be addressed in Section~\ref{sec:tuning}. The least-square estimator for the TBM~\eqref{eq:model} is
\begin{equation}\label{eq:estimate}
\hat \Theta=\argmin_{\Theta\in\tP}\left\{ -2\langle \tY,\Theta\rangle + \FnormSize{}{\Theta}^2\right\}.
\end{equation}
The objective is equal (ignoring constants) to the sum of squares $\FnormSize{}{\tY-\Theta}^2$ and hence the name of our estimator. 

\section{Statistical convergence}\label{sec:theory}
%\vspace{-.1cm}
In this section, we establish the convergence rate of the least-squares estimator~\eqref{eq:estimate} \DIFdelbegin \DIFdel{. }\DIFdelend \DIFaddbegin \DIFadd{for two measurements.
}

\DIFadd{The first measurement is mean squared error (MSE):
}\begin{equation} \DIFadd{\label{eq:MSE}
\text{MSE}(\trueT,\hat \Theta)=}{\DIFadd{1\over \prod_k d_k}}\DIFadd{\FnormSize{}{\trueT-\hat \Theta}^2,
}\end{equation}
\DIFadd{where $\trueT, \hat \Theta\in\tP$ are the true and estimated mean tensors, respectively.
}



\DIFaddend While the loss function corresponds to the likelihood for the Gaussian tensor model, the same assertion does not hold for other types of distribution such as stochastic tensor block model. Surprisingly, we will show that, with very high probability, \DIFaddbegin \DIFadd{comparing with Gaussian tensor model, }\DIFaddend a simple least-square estimator achieves a \DIFdelbegin \DIFdel{nearly optimal }\DIFdelend \DIFaddbegin \DIFadd{equally high }\DIFaddend convergence rate in a general class of block tensor models.  



\DIFdelbegin \DIFdel{We define the estimation accuracy using the mean squared error (MSE):
}\begin{displaymath}\DIFdel{%DIFDELCMD < \label{eq:MSE}%%%
\text{MSE}(\trueT,\hat \Theta)=}{\DIFdel{1\over \prod_k d_k}}\DIFdel{\FnormSize{}{\trueT-\hat \Theta}^2,
}\end{displaymath}
%DIFAUXCMD
\DIFdel{where $\trueT, \hat \Theta\in\tP$ are the true and estimated mean tensors, respectively.
}%DIFDELCMD < 

%DIFDELCMD < \begin{theorem}[Convergence rate] %DIFDELCMD < \label{thm:main}%%%
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \begin{theorem}[Convergence rate of MSE] \label{thm:mse}
\DIFaddend Let $\hat \Theta$ be the least-square estimator of $\trueT$ under model~\eqref{eq:model}. There exists two constants $C_1, C_2>0$ such that,  
\begin{equation} \label{eq:bound}
\text{MSE}(\trueT,\hat \Theta)\leq {C_1 \sigma^2\over  \prod_k d_k} \left(\prod_k R_k+\sum_k d_k \log R_k\right)
\end{equation}
holds with probability at least $1-\exp(-C_2\sum_k R_k+\sum_k d_k\log R_k)$ uniformly over $\trueT\in\tP$ and all error distribution satisfying~\eqref{eq:noise}. 
\end{theorem}

The convergence rate \DIFaddbegin \DIFadd{of MSE }\DIFaddend in~\eqref{eq:bound} consists of two parts. The first part $\prod_k R_k$ is the number of parameters in the core tensor $\tC$, while the second part $\sum_k d_k \log R_k$ reflects the the complexity for estimating $\mM_k$'s. It is the price that one has to pay for not knowing the locations of the blocks.  

We compare our bound with existing literature. The Tucker tensor decomposition has a minimax convergence rate proportional to $\sum_kd_kR'_k$~\cite{zhang2018tensor}, where $R'_k$ is the multilinear rank in the mode $k$. Applying Tucker decomposition to the TBM yields $\sum_kd_kR_k$, because the mode-$k$ rank is bounded by the number of mode-$k$ clusters. Now, as both the dimension $d_{\min}=\min_kd_k$ and clustering size $R_{\min}=\min_k R_k$ tend to infinity, we have $\prod_k R_k+ \sum_k d_k \log R_k\ll \sum_k d_k R_k$. Therefore, by fully exploiting the block structure, we obtain a better convergence rate than previously possible. 

Recently, ~\cite{chi2018provable} proposed a convex relaxation for estimating the TBM. In the special case when the tensor dimensions are equal at every mode $d_1=\ldots=d_K=d$, their estimator has a convergence rate of order $\tO(d^{-1})$ for all $K\geq 2$. As we see from~\eqref{eq:bound}, our estimate obtains a much better convergence rate $\tO(d^{-(K-1)})$, which is especially favorable as the order increases. 


The bound~\eqref{eq:bound} generalizes the previous results on structured matrix estimation in network analysis~\cite{gao2016optimal,gao2018minimax}. %The optimal convergence rate for estimating the (matrix) stochastic block model was $R_1R_2+d_1\log R_1+d_2\log R_2$~\cite{gao2016optimal}, which fits into our special case when $K=2$. 
Earlier work~\cite{gao2018minimax} suggests the following heuristics on the sample complexity for the matrix case:
\begin{equation} \label{eq:intuition}
{\text{(number of parameters)}+ \log \text{(complexity of models)}\over \text{number of samples}}.
\end{equation}
Our result supports this important principle for general $K\geq 2$. Note that, in the TBM, the sample size is the total number of entries $\prod_k d_k$, the number of parameters is $\prod_k R_k$, and the combinatoric complexity for estimating block structure is of order $\prod_k R_k^{d_k}$. 
%The principle~\eqref{eq:intuition} thus provide an intuition for~\eqref{eq:bound}. 

\DIFdelbegin \DIFdel{We next study the clustering consistency of our method. Let $\mM_k, \mM'_k$ }\DIFdelend %DIF > New paragraph edited by Yuchen-----------------------------------
\DIFaddbegin \DIFadd{Next we study the convergence rate of misclassification rate (MCR). To define the MCR, we let $\mM_k, \hat{\mM}_k$ }\DIFaddend be two membership matrices in the mode $k$\DIFdelbegin \DIFdel{. We define the misclassification rate as $\text{MCR}(\mM_k,\mM'_k)=d_k^{-1}\sum_{i\in[d_k]}\mathds{1}\{\hat \mM_k(i)=\mM'_k(i)\}$. Here $\mM_k(i)$ }\DIFdelend \DIFaddbegin \DIFadd{, and $\mD^{(k)}=\entry{D_{r_k,r_k'}^{(k)}}$ be the mode-$k$ confusion matrix with element $D_{r_k,r'_k}^{(k)}=\frac{1}{d_k}\sum_{i=1}^{d_k}\mathbb{I}\{m_{i,r_k}^{(k)}=r_k,\hat{m}_{i,r'_k}^{(k)}=r'_k\}$, $r,r'\in[R_k]$. Note that the row sums }\DIFaddend (respectively, \DIFdelbegin \DIFdel{$\mM_k'(i)$) denotes the cluster label that entry $i$ belongs to, based on the partition induced }\DIFdelend \DIFaddbegin \DIFadd{column sums) of $\mD^{(k)}$ represent the cluster proportions defined }\DIFaddend by $\mM_k$ (respectively, \DIFdelbegin \DIFdel{$\mM_k'$). }%DIFDELCMD < \begin{theorem}[Clustering consistency]%DIFDELCMD < \label{thm:partition}%%%
%DIFDELCMD < %%%
\DIFdel{Consider a TBM with the core tensor $\tC$ satisfying Assumption~\ref{ass:core}}\DIFdelend \DIFaddbegin \DIFadd{$\hat{\mM_k}$). We restrict ourself to only non-degenerating clusters; that is each row sum and column sum are lowered bounded by $\tau>0$. We denote it as marginal assumption. 
%DIF > Here $c^{(k)}_{i_k}$ refers to the true label of $i_k$th fiber in mode $k$ and $\hc^{(k)}_{i_k}$ refers to the assigned label of $i_k$th fiber in mode $k$, $k\in[K]$, $r_k,r'_k\in [R_k]$. 
For $k\in[K]$, the MCR is defined as: 
}\begin{equation} \DIFadd{\label{eq:MCR}
\text{MCR}(\mM_k,\hat{\mM}_k) = \max_{r\in [R_k], a\neq a'\in [R_k]}\min\{D^{(k)}_{ar},D^{(k)}_{a'r}\}
}\end{equation}
 %DIF > where $\mM_k(i)$ (respectively, $\mM_k'(i)$) denotes the cluster label that entry $i$ belongs to, based on the partition induced by $\mM_k$ (respectively, $\mM_k'$).

\DIFadd{When $\mD^{(k)}$ is entrywisely closer to permutation of diagonal matrix, MCR in mode $k$ would be closer to 0. 
}


%DIF > Comparing with MSE which is a loss function which measure the loss caused by not only misclassification but also noise, MCR is another kind of loss function which only focus on clustering. To study the misclassification rate of our estimator, the following theorem gives us the convergence rate of MCR in each mode.


\begin{theorem}[Convergence rate of MCR] \label{thm:mcr}
	\DIFadd{Consider a sub-Gaussian tensor block model with sub-Gaussian parameter $\sigma^2$ and cluster proportion lower bounded by $\tau>0$}\DIFaddend . Let $\trueM$ be the true mode-$k$ membership matrix and $\hat \mM_k$ the estimator from~\eqref{eq:estimate}. \DIFdelbegin \DIFdel{As the dimension $d_{\min}$ tend to infinity, the proportions of misclassified entries go to zero in probability; i. e. there }\DIFdelend \DIFaddbegin \DIFadd{Define the minimal gap between the block means as $\delta_{min}=\min\{\displaystyle\min_{r_1\neq r_1'}\max_{r_2,...,r_K}(c_{r_1,...,r_K}-c_{r_1',...,r_K})^2,...,\displaystyle\min_{r_K\neq r_K'}\max_{r_1,...,r_{K-1}}(c_{r_1,...,r_K}-c_{r_1,...,r_K'})^2\}>0$. There }\DIFaddend exist permutation matrices $\mP_k$'s such that
	\DIFdelbegin \[
\DIFdel{\sum_k \text{MCR}(\hat \mM_k, \mP_k\trueM) \rightarrow 0, \quad \text{in probability}.
}\]
%DIFAUXCMD
\DIFdelend \DIFaddbegin \begin{equation} \DIFadd{\label{eq:mcrbound}
	\mathbb{P}(\text{MCR}(\hat{\mM}_k, \mP_k\mM_{k,true})\geq \varepsilon) \leq  2^{1+\sum_{k=1}^Kd_k}\exp{\left(-\frac{C_2\varepsilon^2\delta_{min}^2\tau^{3K-2}\prod_{k=1}^Kd_k}{\sigma^2}\right)}
	}\end{equation}
	\DIFadd{where $C_2$ is a positive constant.
}\DIFaddend \end{theorem}
\DIFdelbegin \DIFdel{The above theorem shows that our estimator consistently recovers the block structure as the dimension of the data tensor grows. 
}\DIFdelend \DIFaddbegin 

\DIFadd{The convergence rate of MCR in }\eqref{eq:mcrbound} \DIFadd{implies that higher minimum gap between tensor block means, higher dimension and lower noise  can lead to higher accuracy of clustering. 
}\DIFaddend 

\DIFaddbegin \DIFadd{The Table~\ref{tab:addcom} summarizes the comparison with other alternative methods. 
}\DIFaddend \vspace{-.1cm}
\DIFaddbegin \begin{table}[h!]
	\centering
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{c|ccc|c}\hline
			Method&Recovery error (MSE) & Clustering error (MCR)& Block detection& Time complexity (flop / iter)\\
			\hline
			Tucker (rigorous; $K=3$) & $dR$&-& No&$d^K$ \\
			CoCo [Chi et al 2018] (rigorous) &$d^{K-1}$&-&No& $d^{K+1}$ or $d^K$\\
			TBM (rigorous, this paper) &$d\log R$& $ \frac{\sigma\sqrt{K}}{\delta_{min}d^{(k-1)/2}\tau^{(3K-2)/2}} $&Yes&$d^K$\\
			Optimal rate [Gao et al 2018] (heuristic)& $d\log R$&-&-&-\\\hline
		\end{tabular}

	}
	\caption{\DIFaddFL{Summary of the comparison among different tensor clustering methods when $d_1=...=d_K=d, R_1=...=R_K=R$.}}
	\label{tab:addcom}
\end{table}
\vspace{-.3cm}




%DIF >  We next study the clustering consistency of our method. 
%DIF >  \begin{theorem}[Clustering consistency]\label{thm:partition}
%DIF >  Consider a TBM with the core tensor $\tC$ satisfying Assumption~\ref{ass:core}. Under the same definition of $\mM_{k,true}$ and  $\hat{\mM}_k$, as the dimension $d_{\min}$ tend to infinity, the proportions of misclassified entries go to zero in probability; i.e. there exist permutation matrices $\mP_k$'s such that
%DIF >  \[
%DIF >  \sum_k \text{MCR}(\hat \mM_k, \mP_k\trueM) \rightarrow 0, \quad \text{in probability}.
%DIF >  \]
%DIF >  \end{theorem}
%DIF >  The above theorem shows that our estimator consistently recovers the block structure as the dimension of the data tensor grows. 



\vspace{-.1cm}
\DIFaddend \section{Numerical implementation}
\subsection{Alternating optimization}
We introduce an alternating optimization for solving~\eqref{eq:estimate}. Estimating $\Theta$ consists of finding both the core tensor $\tC$ and the membership matrices $\mM_k$'s. The optimization~\eqref{eq:estimate} can be written as
\begin{align}\label{eq:op}
(\hat \tC, \{\hat \mM_k\})&=\argmin_{\tC\in\mathbb{R}^{R_1\times \cdots \times R_K}, \text{ membership matrices $\mM_k$'s}} f(\tC,\{\mM_k\}),\notag\\
 \text{where}&\quad f(\tC,\{\mM_k\})=\FnormSize{}{\tY-\tC\times_1\mM_1\times_2\ldots\times_K\mM_K}^2.\notag
\end{align}
The decision variables consist of $K+1$ blocks of variables, one for the core tensor $\tC$ and $K$ for the membership matrices $\mM_k$'s. We notice that, if any $K$ out of the $K+1$ blocks of variables are known, then the last block of variables can be solved explicitly. This observation suggests that we can iteratively update one block of variables at a time while keeping others fixed. Specifically, given the collection of $\hat \mM_k$'s, the core tensor estimate $\hat \tC=\argmin_{\tC}f(\tC,\{\hat \mM_k\})$ consists of the sample averages of each tensor block. Given the block mean $\hat \tC$ and $K-1$ membership matrices, the last membership matrix can be solved using a simple nearest neighbor search over only $R_k$ discrete points. The full procedure is described in Algorithm~\ref{alg:B}.

\vspace{-.1cm}
\begin{algorithm}
\caption{Multiway clustering based on tensor block models}\label{alg:B}
\begin{algorithmic}[1]
\INPUT Data tensor $\tY\in \mathbb{R}^{d_1\times \cdots \times d_K}$, clustering size $\mR=(R_1,\ldots,R_K)$.
\OUTPUT Block mean tensor $\hat \tC\in\mathbb{R}^{R_1\times \cdots\times R_K}$, and the membership matrices $\hat \mM_k$'s. 
\State Initialize the marginal clustering by performing independent $k$-means on each of the $K$ modes.
\Repeat
\State Update the core tensor $\hat \tC=\entry{\hat c_{r_1,\ldots,r_K}}$. Specifically, for each $(r_1,\ldots,r_K)\in[R_1]\times \cdots [R_K]$,
\begin{equation}\label{eq:ols}
\hat c_{r_1,\ldots,r_K}={1\over n_{r_1,\ldots,r_K}}\sum_{\hat \mM_1^{-1}(r_1)\times\cdots\times \hat \mM_K^{-1}(r_K)}y_{i_1,\ldots,i_K},
\end{equation}
where $\mM^{-1}_k(r_k)$ denotes the indices that belong to the $r_k$th cluster in the mode $k$, and $n_{r_1,\ldots,r_K}=\prod_k|\hat \mM_k^{-1}(r_k)|$ denotes the number of entries in the block indexed by $(r_1,\ldots,r_K)$. 
%\State Update membership matrices $\hat \mM_k$'s:
\For{$k$ in $\{1,2,...,K\}$}
\State Update the mode-$k$ membership matrix $\hat \mM_k$. Specifically, for each $a\in[d_k]$, assign the cluster label $\hat \mM_k(a)\in[R_k]$:
	\[
\hat \mM_k(a)=\argmin_{r\in[R_k]}\sum_{\mI_{-k}}\left(\hat c_{\hat \mM_1(i_1),\ldots,r,\ldots,\hat \mM_K(i_K)}-y_{i_1,\ldots,a,\ldots,i_K}\right)^2,
\]
where $\mI_{-k}=(i_1,\ldots,i_{k-1},i_{k+1},\ldots,i_K)$ denotes the tensor coordinates except the $k$-th mode. 
\EndFor
\Until{Convergence} 
\end{algorithmic}
\end{algorithm}
\vspace{-.2cm}

Algorithm~\ref{alg:B} can be viewed as a higher-order extension of the ordinary (one-way) $k$-means algorithm. The core tensor $\tC$ serves as the role of centroids. As each iteration reduces the value of the objective function, which is bounded below, convergence of the algorithm is guaranteed. The per-iteration computational cost scales linearly with the sample size, $d=\prod_k d_k$, and this complexity matches the classical tensor methods~\cite{anandkumar2014tensor,wang2017tensor,zhang2018tensor}. We recognize that obtaining the global optimizer for such a non-convex optimization is typically difficult~\cite{aloise2009np,zhou2013tensor}. Following the common practice in non-convex optimization~\cite{zhou2013tensor}, we run the algorithm multiple times, using random initializations with independent one-way $k$-means on each of the modes. 

\subsection{Tuning parameter selection}\label{sec:tuning}
\vspace{-.1cm}
Algorithm~\ref{alg:B} takes the number of clusters $\mR$ as an input. In practice such information is often unknown and $\mR$ needs to be estimated from the data $\tY$. We propose to select this tuning parameter using Bayesian information criterion (BIC), 
\begin{equation}\label{eq:BIC}
\text{BIC}(\mR) =  \log\left(\FnormSize{}{\tY-\hat \Theta}^2\right)+{ \sum_k \log d_k \over \prod_k d_k}p_e,
\end{equation}
where $p_e$ is the effective number of parameters in the model. In our case we take $p_e=\prod_k R_k+\sum_k d_k\log R_k$, which is inspired from~\eqref{eq:intuition}. We choose $\hat \mR$ that minimizes $\text{BIC}(\mR)$ via grid search. Our choice of BIC aims to balance between the goodness-of-fit for the data and the degree of freedom in the population model. We test its empirical performance in Section~\ref{sec:simulation}.  

\vspace{-.1cm}
\section{Extension to sparse estimation}
\vspace{-.2cm}
In some large-scale applications, not every block in a data tensor is of equal importance. For example, in the \DIFdelbegin \DIFdel{genome-wise }\DIFdelend \DIFaddbegin \DIFadd{genome-wide }\DIFaddend expression data analysis, only a few entries represent the signals while the majority come from the background noise (see Figure~\ref{fig:1}b). %Another example is community detection in a large sparse network, where only a few blocks represent the communities of interest and others represent small, noisy groups with weak connection.  
While our estimator~\eqref{eq:estimate} is still able to handle this scenario by assigning small values to some of the $\hat c_{r_1,\ldots,r_K}$'s, the estimates may suffer from high variance. It is thus beneficial to introduce regularized estimation for better bias-variance trade-off and improved interpretability. 

Here we illustrate the regularized TBM using \emph{sparsity} on the block means for localizing important blocks in the data tensor. This problem can be formulated as a variable selection on the block parameters. We propose the following regularized least-square estimation:
\[
\hat \Theta^{\text{sparse}}=\argmin_{\Theta\in \tP}\left\{\FnormSize{}{\tY-\Theta}^2+\lambda \normSize{}{\tC}_\rho
\right\},
\]
where $\tC\in\mathbb{R}^{R_1\times \cdots\times R_K}$ is the block-mean tensor, $\normSize{}{\tC}_\rho$ is the penalty function with $\rho$ being an index for the tensor norm, and $\lambda$ is the penalty tuning parameter. Some widely used penalties include Lasso penalty $(\rho=1)$, sparse subset penalty $(\rho=0)$, ridge penalty $(\rho=\text{Frobenius norm})$, elastic net (linear combination of $\rho=1$ and $\rho=\text{Frobenius norm}$), among many others. 

For parsimony purpose, we only discuss the Lasso and sparse subset penalties; other penalizations can be derived similarly. Sparse estimation incurs slight changes to Algorithm~\ref{alg:B}. When updating the core tensor $\tC$ in~\eqref{eq:ols}, we fit a penalized least square problem with respect to $\tC$. 
%\[
%\hat \tC^{\text{sparse}}=\argmin_{\tC\in\mathbb{R}^{R_1\times \cdots\times R_K}} \FnormSize{}{\tY-\tC\times_1\hat \mM_1\times\cdots\times_K \hat \mM_K}^2+\lambda \normSize{}{\tC}_\rho.
%\]
The closed form for the entry-wise sparse estimate $\hat c^{\text{sparse}}_{r_1,\ldots,r_K}$ is (see Lemma~\ref{prop:sparse} in the Supplements):
\begin{equation}\label{eq:lasso}
\hat c^{\text{sparse}}_{r_1,\ldots,r_K}=
\begin{cases}
\hat c^{\text{ols}}_{r_1,\ldots,r_K}\mathds{1}\left\{|\hat c^{\text{ols}}_{r_1,\ldots,r_K} |\geq {\sqrt{\lambda \over n_{r_1,\ldots,r_K}}}\right\} & \text{if}\ \rho=0,\\
\text{sign}(\hat c^{\text{ols}}_{r_1,\ldots,r_K})\left(| \hat c^{\text{ols}}_{r_1,\ldots,r_K}|-{\lambda \over 2n_{r_1,\ldots,r_K}}  \right)_{+} &\text{if}\ \rho=1,
\end{cases}
\end{equation}
where $a_{+}=\max(a,0)$ and $\hat c^{\text{ols}}_{r_1,\ldots,r_K}$ denotes the ordinary least-square estimate in~\eqref{eq:ols}. The choice of penalty $\rho$ often depends on the study goals and interpretations in specific applications. Given a penalty function, we select the tuning parameter $\lambda$ via BIC~\eqref{eq:BIC}, where we modify $p_e$ into $p^{\text{sparse}}_e=\normSize{}{\hat \tC^{\text{sparse}}}_0+\sum_kd_k\log R_k$. Here $\normSize{}{\cdot}_0$ denotes the number of non-zero entries in the tensor. The empirical performance of this proposal will be evaluated in Section~\ref{sec:simulation}. 

\vspace{-.2cm}
\section{Experiments}\label{sec:simulation}
\vspace{-.2cm}
In this section, we evaluate the empirical performance of our TBM method. We consider both non-sparse and sparse tensors, and compare the recovery accuracy with other tensor-based methods. Unless otherwise stated, we generate order-3 tensors under the Gaussian tensor block model~\eqref{eq:model}. The block means are generated from  i.i.d.\ Uniform[-3,3]. The entries in the noise tensor $\tE$ are generated from i.i.d.\ Gaussian $(0,\sigma^2)$. In each simulation study, we report the summary statistics across $n_{\text{sim}}=50$ replications. 
\DIFaddbegin 

\DIFaddend %For sparsity tensor, we further consider the following three statistics:
%(3) Total Correct Rate: 1 - the proportion of misjudgment while determining whether the mean signal is zero.;\\
%(4) Correct Zero Rate: the proportion of zero elements are correctly identified in the underlying mean tensor;\\

\subsection{Finite-sample performance}
\vspace{-.2cm}
\DIFaddbegin 

\DIFaddend %We generate noisy order-3 tensors under the tensor block model~\eqref{eq:model}. We consider various values of dimension $\md=(d_1,d_2,d_3)$ and clustering size $\mR=(R_1,R_2,R_3)$ as we described below. Along each mode, the tensor entries are randomly assigned into clusters with uniform probability. The block means are generated i.i.d.\ from Unif[-3,3]. The entries in the noise tensor $\tE$ are generated from i.i.d.\ Gaussian $(0,\sigma^2)$. In each simulation study, we report the summary statistics across $n_{\text{sim}}=50$ replications. 

In the first experiment, we assess the empirical relationship between the root mean squared error (RMSE) and the dimension. We set $\sigma=3$ and consider \DIFdelbegin \DIFdel{four different $\mR$ }\DIFdelend \DIFaddbegin \DIFadd{two  $\mK$ }\DIFaddend settings (see Figure~\ref{fig:RMSE}). \DIFdelbegin \DIFdel{We }\DIFdelend \DIFaddbegin \DIFadd{In the case of $\mK=3$, we }\DIFaddend increase $d_1$ from 20 to 70, and for each choice of $d_1$, we set the other two dimensions $(d_2,d_3)$ such that $d_1\log R_1\approx d_2\log R_2\approx d_3\log R_3$. Recall that our theoretical analysis suggests a convergence rate \DIFdelbegin \DIFdel{$\sqrt{\log R_1/d_2d_3}$ }\DIFdelend \DIFaddbegin \DIFadd{$\tO(\sqrt{\log R_1/d_2d_3})$ }\DIFaddend for our estimator. Figure~\ref{fig:RMSE}a plots the recovery error versus the \DIFdelbegin \DIFdel{dimension $d_1$. After rescaling the x-axis as in Figure~\ref{fig:RMSE}b, we }\DIFdelend \DIFaddbegin \DIFadd{rescaled sample size $N_1=\sqrt{d_2d_3/\log R_1}$. We }\DIFaddend find that the RMSE decreases roughly at the rate of \DIFdelbegin \DIFdel{$1/N$, where $N=\sqrt{d_2d_3/\log R_1}$ is the rescaled sample size}\DIFdelend \DIFaddbegin \DIFadd{$1/N_1$}\DIFaddend . This is consistent to our theoretical result. It is observed that tensors with a higher number of blocks tend to yield higher recovery errors, as reflected by the upward shift of the curves as $\mR$ increases. Indeed, a higher $\mR$ means a higher intrinsic dimension of the problem, thus increasing the difficulty of the estimation. \DIFaddbegin \DIFadd{Similar behavior can be observed in the $\mK=4$'s case from Figure~\ref{fig:RMSE}b, where the rescaled sample size $N_2 = \sqrt{d_2d_3d_4/logR_1}$.
}\DIFaddend 


\begin{figure}[h!]
\vspace{-.1cm}
\centering
\DIFdelbeginFL %DIFDELCMD < \includegraphics[width=.8\textwidth]{figures/decay.pdf}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[width=.4\textwidth]{figures/figure.pdf}
\includegraphics[width=.4\textwidth]{figures/rescale_order4.pdf}
\DIFaddendFL \caption{\small Estimation error for block tensors with Gaussian noise. Each curve corresponds to a fixed clustering size $\mR$. (a) Average RMSE against \DIFdelbeginFL \DIFdelFL{$d_1$}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{rescaled sample size $N_1=\sqrt{d_2d_3/\log R_1}$ when tensor is of 3-order}\DIFaddendFL . (b) Average RMSE against rescaled sample size \DIFdelbeginFL \DIFdelFL{$N=\sqrt{d_2d_3/\log R_1}$}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{$N_2=\sqrt{d_2d_3d_4/\log R_1}$ when tensor is of 4-order}\DIFaddendFL . 
}\label{fig:RMSE}
\vspace{-.2cm}
\end{figure}

In the second experiment, we evaluate the selection performance of our BIC criterion~\eqref{eq:BIC}. Supplementary Table~\ref{tab:rank} reports the selected numbers of clusters under various combinations of dimension $\md$, clustering size $\mR$, and noise $\sigma$. We find that, for the case $\md=(40,40,40)$ and $\mR=(4,4,4)$, the BIC selection is accurate in the low-to-moderate noise setting. In the high-noise setting with $\sigma=12$, the selected number of clusters is slightly smaller than the true number, but the accuracy increases when either the dimension increases to $\md=(40,40,80)$ or the clustering size reduces to $\mR=(2,3,4)$. Within a tensor, the selection seems to be easier for shorter modes with smaller number of clusters. This phenomenon is to be expected, since shorter mode has more effective samples for clustering. 


\subsection{Comparison with alternative methods}
\vspace{-.2cm}
Next, we compare our TBM method with two popular low-rank tensor estimation methods: (i) CP decomposition and (ii) Tucker decomposition. Following the literature~\cite{chi2018provable,hore2016tensor,kolda2008scalable}, we perform the clustering by applying the $k$-means to the resulting factors along each of the modes. We refer to such techniques as CP+$k$-means and Tucker+$k$-means. 
%\footnote{TBM implementation: https://github.com/wanglab/tensorsparse}.

We generate noisy block tensors with five clusters on each of the modes, and then assess both the estimation and clustering performance for each method. Note that TBM takes a single shot to perform estimation and clustering simultaneously, whereas CP and Tucker-based methods separate these two tasks in two steps. We use the RMSE to assess the estimation accuracy and use the clustering error rate (CER) to measure the clustering accuracy. The CER is calculated using the disagreements (i.e., one minus rand index) between the true and estimated block partitions in the three-way tensor. For fair comparison, we provide all methods the true number of clusters. 

Figure~\ref{fig4}a shows that TBM achieves the lowest estimation error among the three methods. The gain in accuracy is more pronounced as the noise grows. Neither CP nor Tucker recovers the signal tensor, although Tucker appears to result in a modest clustering performance (Figure~\ref{fig4}b). One possible explanation is that the Tucker model imposes orthogonality to the factors, which make the subsequent $k$-means clustering easier than that for the CP factors. Figure~\ref{fig4}b-c shows that the clustering error increases with noise but decreases with dimension. This agrees with our expectation, as in tensor data analysis, a larger dimension implies a larger sample size. 



\begin{figure}[h!]
\centering
\includegraphics[width=.8\textwidth]{figures/compare}
\caption{\small Performance comparison in terms of RMSE and CER. (a) Estimation error against noise for tensors of dimension $(40,40,40)$. (b) Clustering error against noise for tensors of dimension $(40,40,40)$. (c) Clustering error against noise for tensors of dimension $(40,50,60)$.} \label{fig4}
\vspace{-.2cm}
\end{figure}


\textbf{Sparse case.} We then evaluate the performance when the signal tensor is sparse. The simulated model is the same as before, except that we generate block means from a mixture of zero mass and Uniform[-3,3], with probability $p$ (sparsity rate) and $1-p$ respectively. %DIF < We generate noisy tensors of dimension $\md=(40,40,40)$ with varying levels of sparsity and noise. %The initial clustering size is set $\mR=(3,4,5)$, but the actual number of clusters may be smaller because zero-mean blocks could be merged together. We utilize $\ell0$-penalized TBM and primarily focus on the selection accuracy. 
\DIFdelbegin \DIFdel{The }\DIFdelend \DIFaddbegin \DIFadd{We generate noisy tensors of dimension $\md=(40,40,40)$ with varying levels of sparsity and noise. The initial clustering size is set $\mR=(3,4,5)$, but the actual number of clusters may be smaller because zero-mean blocks could be merged together. We utilize $\ell0$-penalized TBM and primarily focus on the selection accuracy. 
The }\DIFaddend performance is quantified via the the sparsity error rate, which is the proportion of entries that were incorrectly set to zero or incorrectly set to non-zero. We also report the proportion of true zero's that were correctly identified (correct zeros). 

Table~\ref{t5} reports the BIC-selected $\lambda$ averaged across 50 simulations. We see a substantial benefit obtained by penalization. The proposed $\lambda$ is able to guide the algorithm to correctly identify zero's, while maintaining good accuracy in identifying non-zero's. The resulting sparsity level is close to the ground truth. \DIFdelbegin \DIFdel{The rows with $\lambda=0$ correspond to the three non-sparse algorithms (CP, Tucker, and non-sparse TBM). Because non-sparse algorithms fail to identify zero's, they show equally poor performance in all selection metrics. }\DIFdelend Supplementary Figure~\ref{fig:sparse} shows the estimation error and sparsity error against $\sigma$ when \DIFdelbegin \DIFdel{$\rho=0.8$}\DIFdelend \DIFaddbegin \DIFadd{$p=0.8$}\DIFaddend . Again, the sparse TBM outperforms the other methods. 


%DIF >  \begin{table}[h!]
%DIF >  \centering
%DIF >  	 \resizebox{.9\textwidth}{!}{%
%DIF >  	\begin{tabular}{|c|c|c|c|c|c|}
%DIF >  		\hline
%DIF >  		Sparsity ($p$)&Noise $(\sigma)$&Penalization $(BIC-selected \lambda)$&Estimated Sparsity Rate&Correct Zero Rate&Sparsity Error Rate  \\ \hline
%DIF >  		0.5&4&136.0(37.5)&\bf{0.55(0.04)}&\bf{1.00(0.02)}&0.06(0.03)\\\hline
%DIF >  0.5&8&439.2(80.2)&\bf{0.58(0.06)}&\bf{0.94(0.08)}&0.15(0.07)\\\hline
%DIF >  0.8&8&458.0(63.3)&\bf{0.81(0.15)}&\bf{0.87(0.16)}&\bf{0.21(0.13)}\\\hline
\DIFaddbegin 

%DIF >  		\hline
%DIF >  	\end{tabular}
%DIF >  	}
%DIF >  \caption{\small Sparse TBM for estimating tensors of dimension $\md=(40,40,40)$. The reported $\bar \lambda$ is the mean of $\lambda$ selected across 50 simulations using proposed BIC criterion. Number in bold indicates within 1.5 standard deviation away from ground truth.}\label{t5}
%DIF >  \end{table}

\DIFaddend \begin{table}[h!]
\centering
	 \DIFdelbeginFL %DIFDELCMD < \resizebox{.9\textwidth}{!}{%
%DIFDELCMD < 	\begin{tabular}{|c|c|c|c|c|c|}
%DIFDELCMD < 		\hline
%DIFDELCMD < 		Sparsity ($\rho$)&Noise $(\sigma)$&Penalization $(\lambda)$&Estimated Sparsity Rate&Correct Zero Rate&Sparsity Error Rate  \\ \hline
%DIFDELCMD < 		\multirow{2}{*}{0.5}&\multirow{2}{*}{4}&$\lambda=0$&$0(0)$&$0(0)$&$0.49(0.03)$\\
%DIFDELCMD < 		&&$\bar{\lambda}=136.4$&${\bf 0.56(0.04)}$&${\bf 0.99(0.02)}$&$\mathbf{0.06(0.03)}$\\
%DIFDELCMD < 		\hline
%DIFDELCMD < 		\multirow{2}{*}{0.5}&\multirow{2}{*}{8}&$\lambda=0$&0(0)&0(0)&$0.49(0.03)$\\
%DIFDELCMD < 		&&$\bar{\lambda}=439.7$&${\bf 0.59(0.05)}$&${\bf 0.99(0.01)}$&$0.14(0.06)$\\
%DIFDELCMD < 		\hline
%DIFDELCMD < 		\multirow{2}{*}{0.8}&\multirow{2}{*}{8}&$\lambda=0$&$0(0)$&$0(0)$&$0.80(0.05)$\\
%DIFDELCMD < 		&&$\bar{\lambda}=241.3$&${\bf 0.83(0.06)}$&${\bf 0.95(0.04)}$&${\bf 0.12(0.06)}$\\
%DIFDELCMD < 		\hline
%DIFDELCMD < 	\end{tabular}
%DIFDELCMD < 	}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \resizebox{.9\textwidth}{!}{%
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Sparsity ($p$)&Noise $(\sigma)$&Penalization $(BIC-selected \lambda)$&Estimated Sparsity Rate&Correct Zero Rate&Sparsity Error Rate  \\ \hline
		0.5&4&136.0(37.5)&\bf{0.55(0.04)}&\bf{1.00(0.02)}&\bf{0.06(0.03)}\\\hline
0.5&8&439.2(80.2)&\bf{0.58(0.06)}&\bf{0.94(0.08)}&0.15(0.07)\\\hline
0.8&8&458.0(63.3)&\bf{0.81(0.15)}&\bf{0.87(0.16)}&\bf{0.21(0.13)}\\\hline

		\hline
	\end{tabular}
	}
\DIFaddendFL \caption{\small Sparse TBM for estimating tensors of dimension $\md=(40,40,40)$. The reported $\bar \lambda$ is the mean of $\lambda$ selected across 50 simulations using proposed BIC criterion. Number in bold indicates \DIFdelbeginFL \DIFdelFL{no significant difference between }\DIFdelendFL the estimate \DIFdelbeginFL \DIFdelFL{and }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{is within twice }\DIFaddendFL the \DIFaddbeginFL \DIFaddFL{standard deviation away from the }\DIFaddendFL ground truth\DIFdelbeginFL \DIFdelFL{, based on a $z$-test with a level 0.05}\DIFdelendFL .}\label{t5}
%DIF > \caption{\small Sparse TBM for estimating tensors of dimension $\md=(40,40,40)$. The reported $\bar \lambda$ is the mean of $\lambda$ selected across 50 simulations using proposed BIC criterion. Number in bold indicates no significant difference between the estimate and the ground truth, based on a $z$-test with a level 0.05.}\label{t5}
\end{table}

\DIFdelbegin %DIFDELCMD < \vspace{-.4cm}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \vspace{-.3cm}
\DIFaddend \subsection{Real data analysis}
\vspace{-.05cm}
Lastly, we apply our method on two real datasets. %We briefly summarize the main findings here; the detailed information can be found in the Supplements. 
The first dataset is a real-valued tensor, consisting of approximate 1 million expression values from 13 brain tissues, 193 individuals, and 362 genes~\cite{wang2017three}. We subtracted the overall mean expression from the data, and applied the $\ell0$-penalized TBM to identify important blocks in the resulting tensor. The top blocks exhibit a clear tissues $\times$ genes specificity (Supplementary Tabel~\ref{tab:gene}). In particular, the top over-expressed block is driven by tissues \{\emph{Substantia nigra, Spinal cord}\} and genes \{\emph{GFAP, MBP}\}, suggesting their elevated expression across individuals. In fact, \emph{GFAP} encodes filament proteins for mature astrocytes and \emph{MBP} encodes myelin sheath for oligodendrocytes, both of which play important roles in the central nervous system~\cite{o2015reference}. Our method also identifies blocks with extremely negative means (i.e.\ under-expressed blocks). The top under-expressed block is driven by tissues \{\emph{Cerebellum, Cerebellar Hemisphere}\} and genes \{\emph{CDH9, GPR6, RXFP1, CRH, DLX5/6, NKX2-1, SLC17A8}\}. The gene \emph{DLX6} encodes proteins in the forebrain development~\cite{o2015reference}, whereas cerebellum tissues are located in the hindbrain brain. The opposite spatial function is consistent with the observed under-expression pattern. 


The second dataset we consider is the \emph{Nations} data~\cite{nickel2011three}. This is a $14\times 14 \times 56$ binary tensor consisting of 56 political relationships of 14 countries between 1950 and 1965. We note that 78.9\% of the entries are zero. Again, we applied the $\ell0$-penalized TBM to identify important blocks in the data. 
We found that the 14 countries are naturally partitioned into 5 clusters, two representing neutral countries \{\emph{Brazil, Egypt, India, Israel, Netherlands}\} and \{\emph{Burma, Indonesia, Jordan}\}, one eastern bloc \{\emph{China, Cuba, Poland, USSA}\}, and two western blocs, \{\emph{USA}\} and \{\emph{UK}\}. The relation types are partitioned into 7 clusters, among which the exports-related activities \{\emph{reltreaties, book translations, relbooktranslations, exports3, relexporsts}\} and NGO-related activities \{\emph{relintergovorgs, relngo, intergovorgs3, ngoorgs3}\} are two major clusters that involve the connection between neutral and western blocs. Other top blocks are described in the Supplement\DIFaddbegin \DIFadd{. 
}

\DIFadd{Specifically, we ran the clustering analysis on the }\emph{\DIFadd{Brain expression}} \DIFadd{and }\emph{\DIFadd{Nations}} \DIFadd{datasets and then compared the goodness-of-fit of different methods. Because the code of CoCo method }[\DIFadd{Chi et al, 2018}] \DIFadd{is not yet available as of 07/31/2019, we excluded it from our numerical comparison (we did have a theoretical comparison with CoCo). The Table~\ref{tab:add} summarizes the proportion of variance explained by each clustering method:
}

\begin{table}[h!]
\centering
	 \resizebox{0.95\textwidth}{!}{
\begin{tabular}{c|cccccc}
\hline
Dataset& TBM &TBM-sparse&  CP&Tucker & CoTeC [Jegelka et al 2009]&CoCo [Chi et al 2018]\\
\hline

Brain expression&0.856&0.855&0.576&0.434&0.849&-\\
Nations&0.439&0.433&0.324&0.253&0.419&-\\
\hline
\end{tabular}
}
\caption{\DIFaddFL{Comparison of goodness-of-fit in the }\emph{\DIFaddFL{Brain}} \DIFaddFL{expression and }\emph{\DIFaddFL{Nations}} \DIFaddFL{datasets.}}\label{tab:add}
\end{table}
\vspace{-.3cm}

\DIFadd{Our method (TBM) achieves the highest variance proportion, suggesting that the entries within the same cluster are close (i.e., a good clustering). As expected, the sparse TBM results in a slightly lower proportion, because it has a lower model complexity at the cost of small bias. It is remarkable that the sparse TBM still achieves a higher goodness-of-fit than others. The improved interpretability with little loss of accuracy makes the sparse TBM appealing in applications}\DIFaddend . 


\vspace{-.05cm}
\section{Conclusion}
\vspace{-.05cm}
We have developed a statistical setting for studying the tensor block model. Under the assumption that tensor entries are distributed with a block-specific mean, our estimator achieves a convergence rate $\tO( \sum_kd_k\log R_k)$ which is faster than previously possible. Our TBM method applies to a broad range of data distributions and can handle both sparse and sense data tensor. We demonstrate the benefit of sparse regularity in power of detection. In specific applications, prior knowledge may suggest other regularities for parameters. For example, in the multi-layer network analysis, sometimes it may be reasonable to impose symmetry on the parameters along certain modes. In some other applications, non-negativity of parameter values may be enforced. We leave these directions for future study. 


\bibliographystyle{unsrt}
\bibliography{tensor_wang}


\end{document}
